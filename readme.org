* intro
Tools to understand neural representations, and application to
contextualizers.

** motivation
Recently, there have been great advances in NLP multitask and transfer
learning. In multitask learning, the goal is not so much to solve a
particular problem (i.e part of speech tagging), but to learn good
representations (vectors in some high dimensional space). These models
can be viewed as extraordinarily complex feature mappings of words,
which are then used in downstream models. Amazingly, the simple problem
of how to map words to vectors still hasn't been solved.

These are "contextual representations", as opposed to static
representations. This means the representations depend on context. Thus,
it isn't a map from words to vectors, but rather a map from sentence,
position pairs to vectors. 

We are studying these representations. Some models we are studying are
- ELMo
- BERT
- XLnet
- OpenAI gpt-1 and gpt-2
** methodology
As of Fri Aug 16 11:28:57 EDT 2019, we have two distinct methodologies.
- Sorting neurons in a given model by importance (then inspecting those
  neurons)
- Correlating representations
  
We currently call both of these "correlation methods", found in
=corr_methods.py=.
   
** extra
I'll use org files as the default enhanced text file format. Some
subdirectories of the main project dir will also have files named
=readme.org=. 

This directory, =contextual-corr-analysis=, should be on =sys.path=. It
will be if you use conda env =jmw0=, which you can find in
=/usr/users/johnmwu/anaconda3=.

* running
The main script to call is =main.py=. Simplified, its calling format is

~main.py [--methods [METHODS ...]] REPRESENTATION_FILES OUTPUT_FILE~

For specific examples, see the scripts in =slurm=. To see all options,
try ~main.py --help~.
* workflow
Our pipeline is:
1. Generate representations, which are assumed to be hdf5
2. Run =main.py= on them 
   1. Loads the representations (=load_representations= in
      =corr_methods.py=)
   2. Compute the correlations using the given methods
   3. Writes them to =OUTPUT_FILE=. 
3. Analyze results (the OUTPUT_FILE above) in the =analysis= directory
* modifying
New correlation methods should extend =Method=, found in =corr_methods=.

The =var= python module contains things you have a reasonable chance of
wanting to modify.
* dir
I try to be very organized. As much as possible, files are organized in
an if-then-else fashion. What this means is that if directories form a
decision tree on the space of all files, then the subdirectories of a
given one are ordered. Their descriptions (the region of space they
occupy) are not necessarily non-intersecting. It should be understood
that the *first* matching subdirectory will be where the file is placed.

The root directory encompasses the entire space. Thus, all files are
allowed to be placed there (and many miscellaneous ones are). 

** main.py
Script to execute. i.e ./main.py ...

Name may change (Fri Jun 14 15:20:45 EDT 2019)
** misc.org
Default case for relevant information. Contains the weekly updates.
** analysis
Data analysis. The results that will be presented. 

analysis-n is typically the analysis notebook for the results
generated by slurm/mk_resultsn.
** hnb
"Helper notebook." Files in this directory are to
- help me code
- help the reader understand
the resulting .py files.

These are basically files that contain an exact copy of the function,
just with loops denested (run once with an arbitrary value, to help
debugging) and structure broken down.

Temporary files needed by hnb's are also here. 
** tests
Testing correctness of code for bugs. Includes slurm scripts, notebooks
that load in generated results (to see if the generated results work). 
** slurm
SLURM scripts, as well as additional files to aid the scripts. 

Executable scripts here are typically run directly as ~SCRIPTNAME~. The
scripts run in this way are executable files; nonexecutable files are
generally helpers.
