{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "from os.path import basename, dirname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set arguments arbitrarily\n",
    "limit = 10000\n",
    "layerspec_l = [\n",
    "    \"all\", \n",
    "    -1, \n",
    "]\n",
    "first_half_only_l = [\n",
    "    False, \n",
    "    False,\n",
    "]\n",
    "second_half_only_l = [\n",
    "    False,\n",
    "    False\n",
    "]\n",
    "representation_fname_l = [\n",
    "    \"/data/sls/temp/belinkov/contextual-corr-analysis/contextualizers/elmo_original/ptb_pos_dev.hdf5\",\n",
    "    \"/data/sls/temp/belinkov/contextual-corr-analysis/contextualizers/calypso_transformer_6_512_base/ptb_pos_dev.hdf5\"\n",
    "]\n",
    "disable_cuda = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fname2mname(fname):\n",
    "    \"\"\"\n",
    "    \"filename to model name\". \n",
    "    \"\"\"\n",
    "    return basename(dirname(fname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_neurons_d = {} \n",
    "representations_d = {} \n",
    "\n",
    "if not disable_cuda and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# for fname in ... loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop variables\n",
    "ix = 0\n",
    "layerspec = layerspec_l[ix]\n",
    "first_half_only = first_half_only_l[ix]\n",
    "second_half_only = second_half_only_l[ix]\n",
    "fname = representation_fname_l[ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set `activations_h5`, `sentence_d`, `indices`\n",
    "activations_h5 = h5py.File(fname, 'r')\n",
    "sentence_d = json.loads(activations_h5['sentence_to_index'][0])\n",
    "temp = {} # TO DO: Make this more elegant?\n",
    "for k, v in sentence_d.items():\n",
    "    temp[v] = k\n",
    "sentence_d = temp # {str ix, sentence}\n",
    "indices = list(sentence_d.keys())[:limit]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set `num_layers`, `num_neurons`, `layers`\n",
    "s = activations_h5[indices[0]].shape\n",
    "num_layers = 1 if len(s)==2 else s[0]\n",
    "num_neurons = s[-1]\n",
    "layers = list(range(num_layers)) if layerspec==\"all\" else [layerspec]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.74 s ± 33.8 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "# Set `num_neurons_d`, `representations_d`\n",
    "for layer in layers:\n",
    "    # Create `representations_l`\n",
    "    representations_l = []\n",
    "    for sentence_ix in indices: \n",
    "        # Set `dim`\n",
    "        dim = len(activations_h5[sentence_ix].shape)\n",
    "        if not (dim == 2 or dim == 3):\n",
    "            raise ValueError('Improper array dimension in file: ' +\n",
    "                             fname + \"\\nShape: \" +\n",
    "                             str(activations_h5[sentence_ix].shape))\n",
    "        \n",
    "        # Create `activations`\n",
    "        activations = torch.FloatTensor(activations_h5[sentence_ix][layer] if dim==3 \n",
    "                                        else activations_h5[sentence_ix])\n",
    "        activations = activations.to(device)\n",
    "\n",
    "        # Create `representations`\n",
    "        representations = activations\n",
    "        if first_half_only: \n",
    "            representations = torch.chunk(representations, chunks=2,\n",
    "                                          dim=-1)[0]\n",
    "        elif second_half_only:\n",
    "            representations = torch.chunk(representations, chunks=2,\n",
    "                                          dim=-1)[1]\n",
    "\n",
    "        representations_l.append(representations)\n",
    "    \n",
    "    # update\n",
    "    model_name = \"{model}_{layer}\".format(model=fname2mname(fname), \n",
    "                                          layer=layer)\n",
    "    num_neurons_d[model_name] = num_neurons\n",
    "    representations_d[model_name] = torch.cat(representations_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:10,  6.58s/it]\n"
     ]
    }
   ],
   "source": [
    "# full\n",
    "for loop_var in tqdm(zip(representation_fname_l, layerspec_l, first_half_only_l, second_half_only_l)):\n",
    "    fname, layerspec, first_half_only, second_half_only = loop_var\n",
    "\n",
    "    # Set `activations_h5`, `sentence_d`, `indices`\n",
    "    activations_h5 = h5py.File(fname, 'r')\n",
    "    sentence_d = json.loads(activations_h5['sentence_to_index'][0])\n",
    "    temp = {} # TO DO: Make this more elegant?\n",
    "    for k, v in sentence_d.items():\n",
    "        temp[v] = k\n",
    "    sentence_d = temp # {str ix, sentence}\n",
    "    indices = list(sentence_d.keys())[:limit]\n",
    "\n",
    "    # Set `num_layers`, `num_neurons`, `layers`\n",
    "    s = activations_h5[indices[0]].shape\n",
    "    num_layers = 1 if len(s)==2 else s[0]\n",
    "    num_neurons = s[-1]\n",
    "    layers = list(range(num_layers)) if layerspec==\"all\" else [layerspec]\n",
    "\n",
    "    # Set `num_neurons_d`, `representations_d`\n",
    "    for layer in layers:\n",
    "        # Create `representations_l`\n",
    "        representations_l = []\n",
    "        for sentence_ix in indices: \n",
    "            # Set `dim`\n",
    "            dim = len(activations_h5[sentence_ix].shape)\n",
    "            if not (dim == 2 or dim == 3):\n",
    "                raise ValueError('Improper array dimension in file: ' +\n",
    "                                 fname + \"\\nShape: \" +\n",
    "                                 str(activations_h5[sentence_ix].shape))\n",
    "\n",
    "            # Create `activations`\n",
    "            activations = torch.FloatTensor(activations_h5[sentence_ix][layer] if dim==3 \n",
    "                                            else activations_h5[sentence_ix])\n",
    "            activations = activations.to(device)\n",
    "\n",
    "            # Create `representations`\n",
    "            representations = activations\n",
    "            if first_half_only: \n",
    "                representations = torch.chunk(representations, chunks=2,\n",
    "                                              dim=-1)[0]\n",
    "            elif second_half_only:\n",
    "                representations = torch.chunk(representations, chunks=2,\n",
    "                                              dim=-1)[1]\n",
    "\n",
    "            representations_l.append(representations)\n",
    "\n",
    "        # update\n",
    "        model_name = \"{model}_{layer}\".format(model=fname2mname(fname), \n",
    "                                              layer=layer)\n",
    "        num_neurons_d[model_name] = num_neurons\n",
    "        representations_d[model_name] = torch.cat(representations_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'elmo_original_0': 1024,\n",
       " 'elmo_original_1': 1024,\n",
       " 'elmo_original_2': 1024,\n",
       " 'calypso_transformer_6_512_base_-1': 1024}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_neurons_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'elmo_original_0': tensor([[-1.4411e-01,  1.0574e+00, -8.1262e-02,  ...,  7.3613e-01,\n",
       "           9.2834e-02, -1.3002e-01],\n",
       "         [ 6.6604e-04, -2.5411e-01, -6.2755e-01,  ..., -6.3302e-02,\n",
       "           1.8540e-01, -1.7286e-01],\n",
       "         [ 1.9155e-01,  2.2999e-01, -2.8944e-01,  ..., -6.4465e-02,\n",
       "           5.8102e-01,  2.1768e-01],\n",
       "         ...,\n",
       "         [-1.0209e-01, -1.4119e-01,  3.2245e-01,  ...,  5.7759e-01,\n",
       "           9.0413e-01, -2.5007e-01],\n",
       "         [-8.8715e-01, -2.0040e-01, -1.0601e+00,  ..., -2.6555e-01,\n",
       "           2.1146e-01,  1.9773e-01],\n",
       "         [-3.1370e-01,  3.0314e-01, -1.9021e-02,  ..., -8.7148e-01,\n",
       "          -4.1681e-01,  3.5492e-01]]),\n",
       " 'elmo_original_1': tensor([[-1.2613e-01,  1.5737e-01, -1.3427e-01,  ..., -8.5516e-01,\n",
       "           3.7451e-01,  1.5019e-01],\n",
       "         [-9.6667e-01, -7.0480e-01, -3.1984e-01,  ..., -5.6957e-01,\n",
       "          -3.0502e-01, -8.0291e-02],\n",
       "         [-6.4364e-01, -7.3775e-01, -1.2226e-01,  ..., -1.4182e-01,\n",
       "           2.7240e-01,  3.8422e-01],\n",
       "         ...,\n",
       "         [-4.0114e-01, -3.3690e-01, -2.7835e-04,  ..., -1.6812e-03,\n",
       "           1.4047e+00, -7.9608e-02],\n",
       "         [-6.3692e-01, -5.1137e-01, -2.3852e-01,  ..., -2.7864e-02,\n",
       "           3.2452e-03, -2.5319e-01],\n",
       "         [-1.0144e-01, -1.7472e-01,  1.3360e-01,  ..., -7.1759e-03,\n",
       "           1.5614e-01,  3.5635e-01]]),\n",
       " 'elmo_original_2': tensor([[-0.0162, -0.4435,  0.0620,  ..., -1.7420,  0.2056,  0.3489],\n",
       "         [-1.2992, -1.9456,  0.4049,  ..., -1.0047, -0.2628,  0.2113],\n",
       "         [-0.8853, -2.1571,  0.1344,  ..., -0.8442,  0.8933,  0.7662],\n",
       "         ...,\n",
       "         [-1.4566, -0.7069,  0.1162,  ...,  0.6021,  2.6262,  0.0305],\n",
       "         [-0.5817, -1.5737, -0.7076,  ...,  0.1753, -0.1922,  0.0349],\n",
       "         [-0.9899, -0.4524,  0.2248,  ...,  0.6204,  0.5664,  1.6108]]),\n",
       " 'calypso_transformer_6_512_base_-1': tensor([[  1.0387,  -3.4095,  -1.5543,  ...,   4.0005,   2.9103,  -2.1940],\n",
       "         [-10.8572,   2.4143,  -0.7933,  ...,   2.0335,   4.9775,  -1.0946],\n",
       "         [  5.7005,  -3.8253,   1.5663,  ...,  -2.0620,  -1.6740,  -1.5838],\n",
       "         ...,\n",
       "         [  2.9296,  -2.4342,   3.2089,  ...,   4.0407,   2.2911,  -0.2821],\n",
       "         [  3.1586,  -1.0164,   5.0472,  ...,   1.7365,   2.2213,   2.3799],\n",
       "         [  3.2334,  -1.7232,   5.1346,  ...,  -2.4711,  -0.3292,   6.8380]])}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "representations_d"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
