{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "from os.path import basename, dirname\n",
    "from var import fname2mname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set arguments arbitrarily\n",
    "limit = 10000\n",
    "layerspec_l = [\n",
    "    \"full\", \n",
    "    -1, \n",
    "]\n",
    "first_half_only_l = [\n",
    "    False, \n",
    "    False,\n",
    "]\n",
    "second_half_only_l = [\n",
    "    False,\n",
    "    False\n",
    "]\n",
    "representation_fname_l = [\n",
    "    \"/data/sls/temp/belinkov/contextual-corr-analysis/contextualizers/elmo_original/ptb_pos_dev.hdf5\",\n",
    "    \"/data/sls/temp/belinkov/contextual-corr-analysis/contextualizers/calypso_transformer_6_512_base/ptb_pos_dev.hdf5\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_neurons_d = {} \n",
    "representations_d = {} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# for fname in ... loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop variables\n",
    "ix = 0\n",
    "layerspec = layerspec_l[ix]\n",
    "first_half_only = first_half_only_l[ix]\n",
    "second_half_only = second_half_only_l[ix]\n",
    "fname = representation_fname_l[ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set `activations_h5`, `sentence_d`, `indices`\n",
    "activations_h5 = h5py.File(fname, 'r')\n",
    "sentence_d = json.loads(activations_h5['sentence_to_index'][0])\n",
    "temp = {} # TO DO: Make this more elegant?\n",
    "for k, v in sentence_d.items():\n",
    "    temp[v] = k\n",
    "sentence_d = temp # {str ix, sentence}\n",
    "indices = list(sentence_d.keys())[:limit]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set `num_layers`, `num_neurons`, `layers`\n",
    "s = activations_h5[indices[0]].shape\n",
    "num_layers = 1 if len(s)==2 else s[0]\n",
    "num_neurons = s[-1]\n",
    "if layerspec == \"all\":\n",
    "    layers = list(range(num_layers))\n",
    "elif layerspec == \"full\":\n",
    "    layers = [\"full\"]\n",
    "else:\n",
    "    layers = [layerspec]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set `num_neurons_d`, `representations_d`\n",
    "for layer in layers:\n",
    "    # Create `representations_l`\n",
    "    representations_l = []\n",
    "    word_count = 0\n",
    "    for sentence_ix in indices: \n",
    "        # Set `dim`, `n_word`, update `word_count`\n",
    "        shape = activations_h5[sentence_ix].shape\n",
    "        dim = len(shape)\n",
    "        if not (dim == 2 or dim == 3):\n",
    "            raise ValueError('Improper array dimension in file: ' +\n",
    "                             fname + \"\\nShape: \" +\n",
    "                             str(activations_h5[sentence_ix].shape))\n",
    "        if dim == 3:\n",
    "            n_word = shape[1]\n",
    "        elif dim == 2:\n",
    "            n_word = shape[0]\n",
    "        word_count += n_word\n",
    "        \n",
    "        # Create `activations`\n",
    "        if layer == \"full\":\n",
    "            activations = torch.FloatTensor(activations_h5[sentence_ix])\n",
    "            if dim == 3:\n",
    "                activations = activations.permute(1, 0, 2)\n",
    "                activations = activations.contiguous().view(n_word, -1)\n",
    "        else:\n",
    "            activations = torch.FloatTensor(activations_h5[sentence_ix][layer] if dim==3 \n",
    "                                                else activations_h5[sentence_ix])\n",
    "\n",
    "        # Create `representations`\n",
    "        representations = activations\n",
    "        if first_half_only: \n",
    "            representations = torch.chunk(representations, chunks=2,\n",
    "                                          dim=-1)[0]\n",
    "        elif second_half_only:\n",
    "            representations = torch.chunk(representations, chunks=2,\n",
    "                                          dim=-1)[1]\n",
    "\n",
    "        representations_l.append(representations)\n",
    "        \n",
    "        # If we've loaded in enough words already, stop\n",
    "        if limit is not None and word_count >= limit:\n",
    "            break\n",
    "    \n",
    "    # update\n",
    "    network = \"{mname}_{layer}\".format(mname=fname2mname(fname), \n",
    "                                          layer=layer)\n",
    "    num_neurons_d[network] = representations_l[0].size()[-1]\n",
    "    representations_d[network] = torch.cat(representations_l)[:limit]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:02,  1.85s/it]\n"
     ]
    }
   ],
   "source": [
    "# full\n",
    "for loop_var in tqdm(zip(representation_fname_l, layerspec_l,\n",
    "                         first_half_only_l, second_half_only_l)):\n",
    "    fname, layerspec, first_half_only, second_half_only = loop_var\n",
    "\n",
    "    # Set `activations_h5`, `sentence_d`, `indices`\n",
    "    activations_h5 = h5py.File(fname, 'r')\n",
    "    sentence_d = json.loads(activations_h5['sentence_to_index'][0])\n",
    "    temp = {} # TO DO: Make this more elegant?\n",
    "    for k, v in sentence_d.items():\n",
    "        temp[v] = k\n",
    "    sentence_d = temp # {str ix, sentence}\n",
    "    indices = list(sentence_d.keys())[:limit]\n",
    "\n",
    "    # Set `num_layers`, `num_neurons`, `layers`\n",
    "    s = activations_h5[indices[0]].shape\n",
    "    num_layers = 1 if len(s)==2 else s[0]\n",
    "    num_neurons = s[-1]\n",
    "    if layerspec == \"all\":\n",
    "        layers = list(range(num_layers))\n",
    "    elif layerspec == \"full\":\n",
    "        layers = [\"full\"]\n",
    "    else:\n",
    "        layers = [layerspec]\n",
    "\n",
    "    # Set `num_neurons_d`, `representations_d`\n",
    "    for layer in layers:\n",
    "        # Create `representations_l`\n",
    "        representations_l = []\n",
    "        word_count = 0\n",
    "        for sentence_ix in indices: \n",
    "            # Set `dim`, `n_word`, update `word_count`\n",
    "            shape = activations_h5[sentence_ix].shape\n",
    "            dim = len(shape)\n",
    "            if not (dim == 2 or dim == 3):\n",
    "                raise ValueError('Improper array dimension in file: ' +\n",
    "                                 fname + \"\\nShape: \" +\n",
    "                                 str(activations_h5[sentence_ix].shape))\n",
    "            if dim == 3:\n",
    "                n_word = shape[1]\n",
    "            elif dim == 2:\n",
    "                n_word = shape[0]\n",
    "            word_count += n_word\n",
    "\n",
    "            # Create `activations`\n",
    "            if layer == \"full\":\n",
    "                activations = torch.FloatTensor(activations_h5[sentence_ix])\n",
    "                if dim == 3:\n",
    "                    activations = activations.permute(1, 0, 2)\n",
    "                    activations = activations.contiguous().view(n_word, -1)\n",
    "            else:\n",
    "                activations = torch.FloatTensor(activations_h5[sentence_ix][layer] if dim==3 \n",
    "                                                    else activations_h5[sentence_ix])\n",
    "\n",
    "            # Create `representations`\n",
    "            representations = activations\n",
    "            if first_half_only: \n",
    "                representations = torch.chunk(representations, chunks=2,\n",
    "                                              dim=-1)[0]\n",
    "            elif second_half_only:\n",
    "                representations = torch.chunk(representations, chunks=2,\n",
    "                                              dim=-1)[1]\n",
    "\n",
    "            representations_l.append(representations)\n",
    "\n",
    "            # If we've loaded in enough words already, stop\n",
    "            if limit is not None and word_count >= limit:\n",
    "                break\n",
    "\n",
    "        # update\n",
    "        network = \"{mname}_{layer}\".format(mname=fname2mname(fname), \n",
    "                                              layer=layer)\n",
    "        num_neurons_d[network] = representations_l[0].size()[-1]\n",
    "        representations_d[network] = torch.cat(representations_l)[:limit]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_representations(representation_fname_l, limit=None,\n",
    "                         layerspec_l=None, first_half_only_l=False,\n",
    "                         second_half_only_l=False):\n",
    "    \"\"\"\n",
    "    Load in representations. Options to control loading exist. \n",
    "\n",
    "    Params:\n",
    "    ----\n",
    "    representation_fname_l : list<str>\n",
    "        List of hdf5 files containing representations\n",
    "    limit : int or None\n",
    "        Limit on number of representations to take\n",
    "    layerspec_l : list\n",
    "        Specification for each model. May be an integer (layer to take),\n",
    "        or \"all\" or \"full\". \"all\" means take all layers. \"full\" means to\n",
    "        concatenate all layers together.\n",
    "    first_half_only_l : list<bool>\n",
    "        Only take the first half of the representations for a given\n",
    "        model.\n",
    "        \n",
    "        If given a single value, will be copied into a list of the\n",
    "        correct length.\n",
    "    second_half_only_l : list<bool>\n",
    "        Only take the second half of the representations for a given\n",
    "        model. \n",
    "\n",
    "        If given a single value, will be copied into a list of the\n",
    "        correct length.\n",
    "\n",
    "    Returns:\n",
    "    ----\n",
    "    num_neuron_d : {str : int}\n",
    "        {network : number of neurons}. Here a network could be a layer,\n",
    "        or the stack of all layers, etc. A network is what's being\n",
    "        correlated as a single unit.\n",
    "    representations_d : {str : tensor}\n",
    "        {network : activations}. \n",
    "    \"\"\"\n",
    "\n",
    "    # Edit args\n",
    "    l = len(representation_fname_l)\n",
    "    if layerspec_l is None:\n",
    "        layerspec_l = ['all'] * l\n",
    "    if type(first_half_only_l) is not list:\n",
    "        first_half_only_l = [first_half_only_l] * l\n",
    "    if type(second_half_only_l) is not list :\n",
    "        second_half_only_l = [second_half_only_l] * l\n",
    "\n",
    "    # Main loop\n",
    "    num_neurons_d = {} \n",
    "    representations_d = {} \n",
    "    for loop_var in tqdm(zip(representation_fname_l, layerspec_l,\n",
    "                             first_half_only_l, second_half_only_l)):\n",
    "        fname, layerspec, first_half_only, second_half_only = loop_var\n",
    "\n",
    "        # Set `activations_h5`, `sentence_d`, `indices`\n",
    "        activations_h5 = h5py.File(fname, 'r')\n",
    "        sentence_d = json.loads(activations_h5['sentence_to_index'][0])\n",
    "        temp = {} # TO DO: Make this more elegant?\n",
    "        for k, v in sentence_d.items():\n",
    "            temp[v] = k\n",
    "        sentence_d = temp # {str ix, sentence}\n",
    "        indices = list(sentence_d.keys())[:limit]\n",
    "\n",
    "        # Set `num_layers`, `num_neurons`, `layers`\n",
    "        s = activations_h5[indices[0]].shape\n",
    "        num_layers = 1 if len(s)==2 else s[0]\n",
    "        num_neurons = s[-1]\n",
    "        if layerspec == \"all\":\n",
    "            layers = list(range(num_layers))\n",
    "        elif layerspec == \"full\":\n",
    "            layers = [\"full\"]\n",
    "        else:\n",
    "            layers = [layerspec]\n",
    "\n",
    "        # Set `num_neurons_d`, `representations_d`\n",
    "        for layer in layers:\n",
    "            # Create `representations_l`\n",
    "            representations_l = []\n",
    "            word_count = 0\n",
    "            for sentence_ix in indices: \n",
    "                # Set `dim`, `n_word`, update `word_count`\n",
    "                shape = activations_h5[sentence_ix].shape\n",
    "                dim = len(shape)\n",
    "                if not (dim == 2 or dim == 3):\n",
    "                    raise ValueError('Improper array dimension in file: ' +\n",
    "                                     fname + \"\\nShape: \" +\n",
    "                                     str(activations_h5[sentence_ix].shape))\n",
    "                if dim == 3:\n",
    "                    n_word = shape[1]\n",
    "                elif dim == 2:\n",
    "                    n_word = shape[0]\n",
    "                word_count += n_word\n",
    "\n",
    "                # Create `activations`\n",
    "                if layer == \"full\":\n",
    "                    activations = torch.FloatTensor(activations_h5[sentence_ix])\n",
    "                    if dim == 3:\n",
    "                        activations = activations.permute(1, 0, 2)\n",
    "                        activations = activations.contiguous().view(n_word, -1)\n",
    "                else:\n",
    "                    activations = torch.FloatTensor(\n",
    "                        activations_h5[sentence_ix][layer] if dim==3 else \n",
    "                        activations_h5[sentence_ix]\n",
    "                    )\n",
    "\n",
    "                # Create `representations`\n",
    "                representations = activations\n",
    "                if first_half_only: \n",
    "                    representations = torch.chunk(\n",
    "                        representations, chunks=2, dim=-1)[0]\n",
    "                elif second_half_only:\n",
    "                    representations = torch.chunk(\n",
    "                        representations, chunks=2, dim=-1)[1]\n",
    "                representations_l.append(representations)\n",
    "\n",
    "                # Early stop\n",
    "                if limit is not None and word_count >= limit:\n",
    "                    break\n",
    "\n",
    "            # Main update\n",
    "            network = \"{mname}_{layer}\".format(mname=fname2mname(fname), \n",
    "                                                  layer=layer)\n",
    "            num_neurons_d[network] = representations_l[0].size()[-1]\n",
    "            representations_d[network] = torch.cat(representations_l)[:limit] \n",
    "    \n",
    "    return num_neurons_d, representations_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:02,  1.81s/it]\n"
     ]
    }
   ],
   "source": [
    "num_neurons_d, representations_d = load_representations(representation_fname_l, limit=10000, layerspec_l=layerspec_l, first_half_only_l=False,\n",
    "                         second_half_only_l=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 1536])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "representations_d['elmo_original-ptb_pos_dev.hdf5_full'].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:36, 24.40s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'elmo_original-ptb_pos_dev.hdf5_full': 3072,\n",
       "  'calypso_transformer_6_512_base-ptb_pos_dev.hdf5_-1': 1024},\n",
       " {'elmo_original-ptb_pos_dev.hdf5_full': tensor([[-1.4411e-01,  1.0574e+00, -8.1262e-02,  ..., -1.7420e+00,\n",
       "            2.0556e-01,  3.4890e-01],\n",
       "          [ 6.6604e-04, -2.5411e-01, -6.2755e-01,  ..., -1.0047e+00,\n",
       "           -2.6278e-01,  2.1131e-01],\n",
       "          [ 1.9155e-01,  2.2999e-01, -2.8944e-01,  ..., -8.4425e-01,\n",
       "            8.9332e-01,  7.6619e-01],\n",
       "          ...,\n",
       "          [-1.0209e-01, -1.4119e-01,  3.2245e-01,  ...,  6.0214e-01,\n",
       "            2.6262e+00,  3.0529e-02],\n",
       "          [-8.8715e-01, -2.0040e-01, -1.0601e+00,  ...,  1.7533e-01,\n",
       "           -1.9222e-01,  3.4938e-02],\n",
       "          [-3.1370e-01,  3.0314e-01, -1.9021e-02,  ...,  6.2039e-01,\n",
       "            5.6637e-01,  1.6108e+00]]),\n",
       "  'calypso_transformer_6_512_base-ptb_pos_dev.hdf5_-1': tensor([[  1.0387,  -3.4095,  -1.5543,  ...,   4.0005,   2.9103,  -2.1940],\n",
       "          [-10.8572,   2.4143,  -0.7933,  ...,   2.0335,   4.9775,  -1.0946],\n",
       "          [  5.7005,  -3.8253,   1.5663,  ...,  -2.0620,  -1.6740,  -1.5838],\n",
       "          ...,\n",
       "          [  2.9296,  -2.4342,   3.2089,  ...,   4.0407,   2.2911,  -0.2821],\n",
       "          [  3.1586,  -1.0164,   5.0472,  ...,   1.7365,   2.2213,   2.3799],\n",
       "          [  3.2334,  -1.7232,   5.1346,  ...,  -2.4711,  -0.3292,   6.8380]])})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_representations(representation_fname_l, limit=None, layerspec_l=layerspec_l, first_half_only_l=first_half_only_l,\n",
    "                         second_half_only_l=second_half_only_l)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:jmw0]",
   "language": "python",
   "name": "conda-env-jmw0-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
