{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "from os.path import basename, dirname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set arguments arbitrarily\n",
    "limit = 10000\n",
    "layerspec_l = [\n",
    "    \"full\", \n",
    "    -1, \n",
    "]\n",
    "first_half_only_l = [\n",
    "    False, \n",
    "    False,\n",
    "]\n",
    "second_half_only_l = [\n",
    "    False,\n",
    "    False\n",
    "]\n",
    "representation_fname_l = [\n",
    "    \"/data/sls/temp/belinkov/contextual-corr-analysis/contextualizers/elmo_original/ptb_pos_dev.hdf5\",\n",
    "    \"/data/sls/temp/belinkov/contextual-corr-analysis/contextualizers/calypso_transformer_6_512_base/ptb_pos_dev.hdf5\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fname2mname(fname):\n",
    "    \"\"\"\n",
    "    \"filename to model name\". \n",
    "    \"\"\"\n",
    "    return '-'.join([basename(dirname(fname)), basename(fname)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_neurons_d = {} \n",
    "representations_d = {} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# for fname in ... loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop variables\n",
    "ix = 0\n",
    "layerspec = layerspec_l[ix]\n",
    "first_half_only = first_half_only_l[ix]\n",
    "second_half_only = second_half_only_l[ix]\n",
    "fname = representation_fname_l[ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set `activations_h5`, `sentence_d`, `indices`\n",
    "activations_h5 = h5py.File(fname, 'r')\n",
    "sentence_d = json.loads(activations_h5['sentence_to_index'][0])\n",
    "temp = {} # TO DO: Make this more elegant?\n",
    "for k, v in sentence_d.items():\n",
    "    temp[v] = k\n",
    "sentence_d = temp # {str ix, sentence}\n",
    "indices = list(sentence_d.keys())[:limit]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set `num_layers`, `num_neurons`, `layers`\n",
    "s = activations_h5[indices[0]].shape\n",
    "num_layers = 1 if len(s)==2 else s[0]\n",
    "num_neurons = s[-1]\n",
    "if layerspec == \"all\":\n",
    "    layers = list(range(num_layers))\n",
    "elif layerspec == \"full\":\n",
    "    layers = [\"full\"]\n",
    "else:\n",
    "    layers = [layerspec]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set `num_neurons_d`, `representations_d`\n",
    "for layer in layers:\n",
    "    # Create `representations_l`\n",
    "    representations_l = []\n",
    "    word_count = 0\n",
    "    for sentence_ix in indices: \n",
    "        # Set `dim`, `n_word`, update `word_count`\n",
    "        shape = activations_h5[sentence_ix].shape\n",
    "        dim = len(shape)\n",
    "        if not (dim == 2 or dim == 3):\n",
    "            raise ValueError('Improper array dimension in file: ' +\n",
    "                             fname + \"\\nShape: \" +\n",
    "                             str(activations_h5[sentence_ix].shape))\n",
    "        if dim == 3:\n",
    "            n_word = shape[1]\n",
    "        elif dim == 2:\n",
    "            n_word = shape[0]\n",
    "        word_count += n_word\n",
    "        \n",
    "        # Create `activations`\n",
    "        if layer == \"full\":\n",
    "            activations = torch.FloatTensor(activations_h5[sentence_ix])\n",
    "            if dim == 3:\n",
    "                activations = activations.permute(1, 0, 2)\n",
    "                activations = activations.contiguous().view(n_word, -1)\n",
    "        else:\n",
    "            activations = torch.FloatTensor(activations_h5[sentence_ix][layer] if dim==3 \n",
    "                                                else activations_h5[sentence_ix])\n",
    "\n",
    "        # Create `representations`\n",
    "        representations = activations\n",
    "        if first_half_only: \n",
    "            representations = torch.chunk(representations, chunks=2,\n",
    "                                          dim=-1)[0]\n",
    "        elif second_half_only:\n",
    "            representations = torch.chunk(representations, chunks=2,\n",
    "                                          dim=-1)[1]\n",
    "\n",
    "        representations_l.append(representations)\n",
    "        \n",
    "        # If we've loaded in enough words already, stop\n",
    "        if limit is not None and word_count >= limit:\n",
    "            break\n",
    "    \n",
    "    # update\n",
    "    model_name = \"{model}_{layer}\".format(model=fname2mname(fname), \n",
    "                                          layer=layer)\n",
    "    num_neurons_d[model_name] = representations_l[0].size()[-1]\n",
    "    representations_d[model_name] = torch.cat(representations_l)[:limit]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:20,  7.32s/it]\n"
     ]
    }
   ],
   "source": [
    "# full\n",
    "for loop_var in tqdm(zip(representation_fname_l, layerspec_l,\n",
    "                         first_half_only_l, second_half_only_l)):\n",
    "    fname, layerspec, first_half_only, second_half_only = loop_var\n",
    "\n",
    "    # Set `activations_h5`, `sentence_d`, `indices`\n",
    "    activations_h5 = h5py.File(fname, 'r')\n",
    "    sentence_d = json.loads(activations_h5['sentence_to_index'][0])\n",
    "    temp = {} # TO DO: Make this more elegant?\n",
    "    for k, v in sentence_d.items():\n",
    "        temp[v] = k\n",
    "    sentence_d = temp # {str ix, sentence}\n",
    "    indices = list(sentence_d.keys())[:limit]\n",
    "\n",
    "    # Set `num_layers`, `num_neurons`, `layers`\n",
    "    s = activations_h5[indices[0]].shape\n",
    "    num_layers = 1 if len(s)==2 else s[0]\n",
    "    num_neurons = s[-1]\n",
    "    if layerspec == \"all\":\n",
    "        layers = list(range(num_layers))\n",
    "    elif layerspec == \"full\":\n",
    "        layers = [\"full\"]\n",
    "    else:\n",
    "        layers = [layerspec]\n",
    "\n",
    "    # Set `num_neurons_d`, `representations_d`\n",
    "    for layer in layers:\n",
    "        # Create `representations_l`\n",
    "        representations_l = []\n",
    "        word_count = 0\n",
    "        for sentence_ix in indices: \n",
    "            # Set `dim`, `n_word`, update `word_count`\n",
    "            shape = activations_h5[sentence_ix].shape\n",
    "            dim = len(shape)\n",
    "            if not (dim == 2 or dim == 3):\n",
    "                raise ValueError('Improper array dimension in file: ' +\n",
    "                                 fname + \"\\nShape: \" +\n",
    "                                 str(activations_h5[sentence_ix].shape))\n",
    "            if dim == 3:\n",
    "                n_word = shape[1]\n",
    "            elif dim == 2:\n",
    "                n_word = shape[0]\n",
    "            word_count += n_word\n",
    "\n",
    "            # Create `activations`\n",
    "            if layer == \"full\":\n",
    "                activations = torch.FloatTensor(activations_h5[sentence_ix])\n",
    "                if dim == 3:\n",
    "                    activations = activations.permute(1, 0, 2)\n",
    "                    activations = activations.contiguous().view(n_word, -1)\n",
    "            else:\n",
    "                activations = torch.FloatTensor(activations_h5[sentence_ix][layer] if dim==3 \n",
    "                                                    else activations_h5[sentence_ix])\n",
    "\n",
    "            # Create `representations`\n",
    "            representations = activations\n",
    "            if first_half_only: \n",
    "                representations = torch.chunk(representations, chunks=2,\n",
    "                                              dim=-1)[0]\n",
    "            elif second_half_only:\n",
    "                representations = torch.chunk(representations, chunks=2,\n",
    "                                              dim=-1)[1]\n",
    "\n",
    "            representations_l.append(representations)\n",
    "\n",
    "            # If we've loaded in enough words already, stop\n",
    "            if limit is not None and word_count >= limit:\n",
    "                break\n",
    "\n",
    "        # update\n",
    "        model_name = \"{model}_{layer}\".format(model=fname2mname(fname), \n",
    "                                              layer=layer)\n",
    "        num_neurons_d[model_name] = representations_l[0].size()[-1]\n",
    "        representations_d[model_name] = torch.cat(representations_l)[:limit]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_representations(representation_fname_l, limit=None,\n",
    "                         layerspec_l=None, first_half_only_l=False,\n",
    "                         second_half_only_l=False):\n",
    "    def fname2mname(fname):\n",
    "        \"\"\"\n",
    "        \"filename to model name\". \n",
    "        \"\"\"\n",
    "        return '-'.join([basename(dirname(fname)), basename(fname)])\n",
    "\n",
    "    num_neurons_d = {} \n",
    "    representations_d = {} \n",
    "    \n",
    "    for loop_var in tqdm(zip(representation_fname_l, layerspec_l,\n",
    "                             first_half_only_l, second_half_only_l)):\n",
    "        fname, layerspec, first_half_only, second_half_only = loop_var\n",
    "\n",
    "        # Set `activations_h5`, `sentence_d`, `indices`\n",
    "        activations_h5 = h5py.File(fname, 'r')\n",
    "        sentence_d = json.loads(activations_h5['sentence_to_index'][0])\n",
    "        temp = {} # TO DO: Make this more elegant?\n",
    "        for k, v in sentence_d.items():\n",
    "            temp[v] = k\n",
    "        sentence_d = temp # {str ix, sentence}\n",
    "        indices = list(sentence_d.keys())[:limit]\n",
    "\n",
    "        # Set `num_layers`, `num_neurons`, `layers`\n",
    "        s = activations_h5[indices[0]].shape\n",
    "        num_layers = 1 if len(s)==2 else s[0]\n",
    "        num_neurons = s[-1]\n",
    "        if layerspec == \"all\":\n",
    "            layers = list(range(num_layers))\n",
    "        elif layerspec == \"full\":\n",
    "            layers = [\"full\"]\n",
    "        else:\n",
    "            layers = [layerspec]\n",
    "\n",
    "        # Set `num_neurons_d`, `representations_d`\n",
    "        for layer in layers:\n",
    "            # Create `representations_l`\n",
    "            representations_l = []\n",
    "            word_count = 0\n",
    "            for sentence_ix in indices: \n",
    "                # Set `dim`, `n_word`, update `word_count`\n",
    "                shape = activations_h5[sentence_ix].shape\n",
    "                dim = len(shape)\n",
    "                if not (dim == 2 or dim == 3):\n",
    "                    raise ValueError('Improper array dimension in file: ' +\n",
    "                                     fname + \"\\nShape: \" +\n",
    "                                     str(activations_h5[sentence_ix].shape))\n",
    "                if dim == 3:\n",
    "                    n_word = shape[1]\n",
    "                elif dim == 2:\n",
    "                    n_word = shape[0]\n",
    "                word_count += n_word\n",
    "\n",
    "                # Create `activations`\n",
    "                if layer == \"full\":\n",
    "                    activations = torch.FloatTensor(activations_h5[sentence_ix])\n",
    "                    if dim == 3:\n",
    "                        activations = activations.permute(1, 0, 2)\n",
    "                        activations = activations.contiguous().view(n_word, -1)\n",
    "                else:\n",
    "                    activations = torch.FloatTensor(activations_h5[sentence_ix][layer] if dim==3 \n",
    "                                                        else activations_h5[sentence_ix])\n",
    "\n",
    "                # Create `representations`\n",
    "                representations = activations\n",
    "                if first_half_only: \n",
    "                    representations = torch.chunk(representations, chunks=2,\n",
    "                                                  dim=-1)[0]\n",
    "                elif second_half_only:\n",
    "                    representations = torch.chunk(representations, chunks=2,\n",
    "                                                  dim=-1)[1]\n",
    "\n",
    "                representations_l.append(representations)\n",
    "\n",
    "                # If we've loaded in enough words already, stop\n",
    "                if limit is not None and word_count >= limit:\n",
    "                    break\n",
    "\n",
    "            # update\n",
    "            model_name = \"{model}_{layer}\".format(model=fname2mname(fname), \n",
    "                                                  layer=layer)\n",
    "            num_neurons_d[model_name] = representations_l[0].size()[-1]\n",
    "            representations_d[model_name] = torch.cat(representations_l)[:limit]   \n",
    "    \n",
    "    return num_neurons_d, representations_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:03,  2.04s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'elmo_original-ptb_pos_dev.hdf5_full': 3072,\n",
       "  'calypso_transformer_6_512_base-ptb_pos_dev.hdf5_-1': 1024},\n",
       " {'elmo_original-ptb_pos_dev.hdf5_full': tensor([[-1.4411e-01,  1.0574e+00, -8.1262e-02,  ..., -1.7420e+00,\n",
       "            2.0556e-01,  3.4890e-01],\n",
       "          [ 6.6604e-04, -2.5411e-01, -6.2755e-01,  ..., -1.0047e+00,\n",
       "           -2.6278e-01,  2.1131e-01],\n",
       "          [ 1.9155e-01,  2.2999e-01, -2.8944e-01,  ..., -8.4425e-01,\n",
       "            8.9332e-01,  7.6619e-01],\n",
       "          ...,\n",
       "          [-2.0288e-02,  1.6141e-01,  3.3374e-01,  ...,  1.0572e+00,\n",
       "            1.0167e+00, -1.1895e-01],\n",
       "          [ 1.1309e-01,  1.5746e-01,  1.4142e-01,  ...,  3.0812e-01,\n",
       "           -3.9564e-01,  4.3130e-01],\n",
       "          [ 2.4991e-01,  4.4100e-01,  5.2294e-02,  ..., -1.4741e+00,\n",
       "           -4.3560e-02,  7.0020e-01]]),\n",
       "  'calypso_transformer_6_512_base-ptb_pos_dev.hdf5_-1': tensor([[  1.0387,  -3.4095,  -1.5543,  ...,   4.0005,   2.9103,  -2.1940],\n",
       "          [-10.8572,   2.4143,  -0.7933,  ...,   2.0335,   4.9775,  -1.0946],\n",
       "          [  5.7005,  -3.8253,   1.5663,  ...,  -2.0620,  -1.6740,  -1.5838],\n",
       "          ...,\n",
       "          [  8.2190,  -0.3780,   9.9016,  ...,  -2.7197,   1.8375,  -3.1481],\n",
       "          [  7.5602,  -1.7785,   0.4032,  ...,   5.5025,   2.1467,   3.0830],\n",
       "          [  3.2757,  -6.2589,   1.9038,  ...,   7.1897,   3.9921,   3.7778]])})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_representations(representation_fname_l, limit=10000, layerspec_l=layerspec_l, first_half_only_l=first_half_only_l,\n",
    "                         second_half_only_l=second_half_only_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    }
   ],
   "source": [
    "load_representations(representation_fname_l, limit=None, layerspec_l=layerspec_l, first_half_only_l=first_half_only_l,\n",
    "                         second_half_only_l=second_half_only_l)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:jmw0]",
   "language": "python",
   "name": "conda-env-jmw0-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
