{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from tqdm import tqdm\n",
    "from itertools import product as p\n",
    "import json\n",
    "import numpy as np\n",
    "import h5py\n",
    "from os.path import basename, dirname\n",
    "import pickle\n",
    "from var import fname2mname"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_fname_l = [\n",
    "    \"/data/sls/temp/belinkov/contextual-corr-analysis/contextualizers/bert_base_cased/ptb_pos_dev_attn.hdf5\"\n",
    "]\n",
    "limit = None\n",
    "layerspec_l = None\n",
    "ar_mask = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hnb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edit args\n",
    "l = len(attention_fname_l)\n",
    "if layerspec_l is None:\n",
    "    layerspec_l = ['all'] * l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_heads_d = {} \n",
    "attentions_d = {} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arbitrary values for main loop\n",
    "loop_var = attention_fname_l[0], layerspec_l[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname, layerspec = loop_var\n",
    "\n",
    "# Set `attentions_h5`, `sentence_d`, `indices`\n",
    "attentions_h5 = h5py.File(fname, 'r')\n",
    "sentence_d = json.loads(attentions_h5['sentence_to_index'][0])\n",
    "temp = {} # TO DO: Make this more elegant?\n",
    "for k, v in sentence_d.items():\n",
    "    temp[v] = k\n",
    "sentence_d = temp # {str ix, sentence}\n",
    "indices = list(sentence_d.keys())[:limit]\n",
    "\n",
    "# Set `num_layers`, `num_heads`, `layers`\n",
    "s = attentions_h5[indices[0]].shape\n",
    "num_layers = s[0]\n",
    "num_heads = s[1]\n",
    "if layerspec == \"all\":\n",
    "    layers = list(range(num_layers))\n",
    "else:\n",
    "    layers = [layerspec]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arbitrary value for layer loop\n",
    "layer = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "attentions_l = []\n",
    "word_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arbitrary value for sentence_ix loop\n",
    "sentence_ix = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set `dim`, `n_word`, update `word_count`\n",
    "shape = attentions_h5[sentence_ix].shape\n",
    "dim = len(shape)\n",
    "if not (dim == 4):\n",
    "    raise ValueError('Improper array dimension in file: ' +\n",
    "                     fname + \"\\nShape: \" +\n",
    "                     str(attentions_h5[sentence_ix].shape))\n",
    "n_word = shape[2]\n",
    "word_count += n_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create `attentions`\n",
    "if ar_mask:\n",
    "    attentions = np.tril(attentions_h5[sentence_ix][layer])\n",
    "    attentions = attentions/np.sum(attentions, axis=-1, keepdims=True)\n",
    "    attentions = torch.FloatTensor(attentions)\n",
    "else:\n",
    "    attentions = torch.FloatTensor(\n",
    "        attentions_h5[sentence_ix][layer] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update `attentions_l`\n",
    "attentions_l.append(attentions)\n",
    "\n",
    "# Early stop\n",
    "if limit is not None and word_count >= limit:\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_attentions(attention_fname_l, limit=None, layerspec_l=None, ar_mask=False):\n",
    "    \"\"\"\n",
    "    Load in attentions. Options to control loading exist. \n",
    "\n",
    "    Params:\n",
    "    ----\n",
    "    attention_fname_l : list<str>\n",
    "        List of hdf5 files containing attentions\n",
    "    limit : int or None\n",
    "        Limit on number of attentions to take\n",
    "    layerspec_l : list\n",
    "        Specification for each model. May be an integer (layer to take),\n",
    "        or \"all\". \"all\" means take all layers. \n",
    "\n",
    "    Returns:\n",
    "    ----\n",
    "    num_head_d : {str : int}\n",
    "        {network : number of heads}. Here a network could be a layer,\n",
    "        or the stack of all layers, etc. A network is what's being\n",
    "        correlated as a single unit.\n",
    "    attentions_d : {str : list<tensor>}\n",
    "        {network : attentions}. attentions is a list because each \n",
    "        sentence may be of different length. \n",
    "    \"\"\"\n",
    "\n",
    "    # Edit args\n",
    "    l = len(attention_fname_l)\n",
    "    if layerspec_l is None:\n",
    "        layerspec_l = ['all'] * l\n",
    "\n",
    "    # Main loop\n",
    "    num_heads_d = {} \n",
    "    attentions_d = {} \n",
    "    for loop_var in tqdm(zip(attention_fname_l, layerspec_l), desc='load'):\n",
    "        fname, layerspec = loop_var\n",
    "\n",
    "        # Set `attentions_h5`, `sentence_d`, `indices`\n",
    "        attentions_h5 = h5py.File(fname, 'r')\n",
    "        sentence_d = json.loads(attentions_h5['sentence_to_index'][0])\n",
    "        temp = {} # TO DO: Make this more elegant?\n",
    "        for k, v in sentence_d.items():\n",
    "            temp[v] = k\n",
    "        sentence_d = temp # {str ix, sentence}\n",
    "        indices = list(sentence_d.keys())[:limit]\n",
    "\n",
    "        # Set `num_layers`, `num_heads`, `layers`\n",
    "        s = attentions_h5[indices[0]].shape\n",
    "        num_layers = s[0]\n",
    "        num_heads = s[1]\n",
    "        if layerspec == \"all\":\n",
    "            layers = list(range(num_layers))\n",
    "        else:\n",
    "            layers = [layerspec]\n",
    "\n",
    "        # Set `num_heads_d`, `attentions_d`\n",
    "        for layer in layers:\n",
    "            # Create `attentions_l`\n",
    "            attentions_l = []\n",
    "            word_count = 0\n",
    "            for sentence_ix in indices: \n",
    "                # Set `dim`, `n_word`, update `word_count`\n",
    "                shape = attentions_h5[sentence_ix].shape\n",
    "                dim = len(shape)\n",
    "                if not (dim == 4):\n",
    "                    raise ValueError('Improper array dimension in file: ' +\n",
    "                                     fname + \"\\nShape: \" +\n",
    "                                     str(attentions_h5[sentence_ix].shape))\n",
    "                n_word = shape[2]\n",
    "                word_count += n_word\n",
    "\n",
    "                # Create `attentions`\n",
    "                if ar_mask:\n",
    "                    attentions = np.tril(attentions_h5[sentence_ix][layer])\n",
    "                    attentions = attentions/np.sum(attentions, axis=-1, keepdims=True)\n",
    "                    attentions = torch.FloatTensor(attentions)\n",
    "                else:\n",
    "                    attentions = torch.FloatTensor(\n",
    "                        attentions_h5[sentence_ix][layer] )\n",
    "\n",
    "                # Update `attentions_l`\n",
    "                attentions_l.append(attentions)\n",
    "\n",
    "                # Early stop\n",
    "                if limit is not None and word_count >= limit:\n",
    "                    break\n",
    "\n",
    "            # Main update\n",
    "            network = \"{mname}_{layer}\".format(mname=fname2mname(fname), \n",
    "                                                  layer=layer)\n",
    "            num_heads_d[network] = attentions_l[0].shape[0]\n",
    "            attentions_d[network] = attentions_l[:limit] \n",
    "    \n",
    "    return num_heads_d, attentions_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "load: 1it [00:35, 35.46s/it]\n"
     ]
    }
   ],
   "source": [
    "num_heads_d, attentions_d = load_attentions(attention_fname_l=attention_fname_l, limit=None, layerspec_l=None, ar_mask=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5588, 0.4412, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1701, 0.1439, 0.6860, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1041, 0.3496, 0.0812, 0.4651, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2647, 0.1313, 0.0475, 0.2357, 0.3209, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0876, 0.1158, 0.1432, 0.1273, 0.4374, 0.0888, 0.0000, 0.0000],\n",
       "        [0.1605, 0.1254, 0.0650, 0.1288, 0.2524, 0.1863, 0.0816, 0.0000],\n",
       "        [0.0377, 0.1092, 0.1972, 0.1351, 0.2546, 0.0715, 0.0824, 0.1122]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fname = attention_fname_l[0]\n",
    "network = \"{mname}_{layer}\".format(mname=fname2mname(fname), layer=0)\n",
    "attentions_d[network][0][0] # first sentence of first head in layer 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "load: 1it [00:32, 32.92s/it]\n"
     ]
    }
   ],
   "source": [
    "num_heads_d, attentions_d = load_attentions(attention_fname_l=attention_fname_l, limit=None, layerspec_l=None, ar_mask=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3232, 0.0535, 0.0561, 0.0233, 0.0566, 0.0113, 0.1845, 0.2914],\n",
       "        [0.1046, 0.0826, 0.1047, 0.1157, 0.1459, 0.0931, 0.1268, 0.2266],\n",
       "        [0.0856, 0.0725, 0.3454, 0.1004, 0.1288, 0.0981, 0.0492, 0.1200],\n",
       "        [0.0255, 0.0855, 0.0199, 0.1137, 0.2686, 0.0544, 0.1137, 0.3187],\n",
       "        [0.1632, 0.0809, 0.0293, 0.1453, 0.1979, 0.0638, 0.1288, 0.1908],\n",
       "        [0.0621, 0.0822, 0.1016, 0.0903, 0.3102, 0.0630, 0.0753, 0.2154],\n",
       "        [0.1291, 0.1009, 0.0523, 0.1036, 0.2030, 0.1498, 0.0657, 0.1956],\n",
       "        [0.0377, 0.1092, 0.1972, 0.1351, 0.2546, 0.0715, 0.0824, 0.1122]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fname = attention_fname_l[0]\n",
    "network = \"{mname}_{layer}\".format(mname=fname2mname(fname), layer=0)\n",
    "attentions_d[network][0][0] # first sentence of first head in layer 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:jmw0]",
   "language": "python",
   "name": "conda-env-jmw0-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
