
Archived entries from file /afs/csail.mit.edu/u/j/johnmwu/core/mind/sys/short/proc/main.org


* DONE Universality and individuality in neural dynamics
  :PROPERTIES:
  :ARCHIVE_TIME: 2019-08-05 Mon 11:16
  :ARCHIVE_FILE: ~/core/mind/sys/short/proc/main.org
  :ARCHIVE_OLPATH: yonatan/contextual-corr-analysis
  :ARCHIVE_CATEGORY: main
  :ARCHIVE_TODO: DONE
  :END:
  on sec 3.3
** DONE Define double centering
** DONE Define MDS
** DONE Summarize
*** CCA based similarity can be misleading, and can depend on the
      activation function.
*** CCA based similarity will cluster based on architecture and
      activation, but the "fixed point topology" and linearized
      dynamics remains similar.
** DONE email them about typo on pg 8
** DONE Are these three tasks standard?
*** K bit flip flop uhh doesn't seem to be
*** sine wave lol... this also comes from some paper written
      earlier by one of the authors
*** CDI doesn't seem to be
** DONE How exactly do they construct this "fixed point topology
    MDS"?
    
    I see. It seems:
*** Construct a markov chain consisting of the fixed points for
      each model
*** Construct a connectivity matrix for the fixed point graph
*** Put all these matrices in the same space, by imposing some
      order on the set of fixed points
      
      Throw out all models which have a non-modal number of fixed
      points.
*** Use euclidean distance (I guess the frobenius norm) to
      quantify dissimilarity.
** DONE How is this constructed for sine wave generation?
    
    Hmm... they don't seem to mention this.
** DONE Add comments to the paper pdf file
** DONE Add into anki
comment
** I feel like their topology analysis is interesting, but sort of
    just confirms the obvious. Yes, all the models are trained on
    the same task, so of course they're learning the same things.
** It would be helpful if they released code. The following things
    are not made clear:
*** How they do fixed point topology MDS (slightly more clear on
      the flip flop task, but even there not exactly)
*** What they mean by eigvals (fig 3e) is the eigvals of some
      section of the jacobian matrix at a fixed point, right?

* DONE I need to learn the standard terminology for all of the nlp
  :PROPERTIES:
  :ARCHIVE_TIME: 2019-08-05 Mon 11:17
  :ARCHIVE_FILE: ~/core/mind/sys/short/proc/main.org
  :ARCHIVE_OLPATH: yonatan/contextual-corr-analysis
  :ARCHIVE_CATEGORY: main
  :ARCHIVE_TODO: DONE
  :END:
 tasks
 
 task
** DONE part of speech tagging
*** DONE Read the initial ptb paper
     
     comment
**** Wait. So do they present the tagset?
**** Interesting.. it seems that papers presenting a dataset go
       into a lot of detail about how the data was collected. It
       makes sense.
**** They talk about the "training of annotators". Isn't this kind
       of like training an organic neural network on a small amt of
       data, to then generate more data for another neural network to
       train on?
**** They talk about annotators having 3 or 4 months of
       experience.. wtf? How long were annotators working on this
       damn thing?
**** This paper renders trees as sexps. Respect.
task
**** DONE Add into anki
**** DONE
*** DONE
** DONE Chunking
*** DONE Read the conll-2000 shared task paper
*** DONE Enter "Chunking" into anki
** DONE named entity recognition
*** DONE Read the conll-2003 paper
** DONE natural language inference
*** DONE Read the SNLI paper
**** DONE OK... what is GloVe?
       
       Oh shit. It'd be cool to have a look at these embeddings.
       
       task
***** DONE Try looking at some GloVe vectors?
****** DONE Get jupyter notebook working from within emacs. Ahh
           fuck. It needs 25.3, which ubuntu doesn't have.
****** DONE Move the misc notebook into my filesystem
***** DONE Add into anki
**** DONE
*** DONE
** DONE coreference resolution
   
   task
*** DONE Read conll 2012
     
     Interesting
**** chinese word segmentation is fairly accurate (pg 9)
task
**** DONE Wait so what are these shared tasks? Ok... so it seems
       that each year there is a shared task.
*** DONE Define "coreference resolution"
** DONE

* DONE Read Yonatan's paper with Nelson
  :PROPERTIES:
  :ARCHIVE_TIME: 2019-08-05 Mon 11:17
  :ARCHIVE_FILE: ~/core/mind/sys/short/proc/main.org
  :ARCHIVE_OLPATH: yonatan/contextual-corr-analysis
  :ARCHIVE_CATEGORY: main
  :ARCHIVE_TODO: DONE
  :END:
 
 What do they mean by "transferable"?
 
 What do they mean by "task trained" LSTM in section 4.2?
 
 comment
** the list of tasks should probably be put in an appendix.
** It's hard to exactly say the main points of this paper..
main points
** language modeling is the most transferable task
   
   evid
*** sec 6.2 and table 3
** lower layers of LSTMs are more transferable, but this is not true
   of transformers
   
   evid
*** figure 3 and 4
** learning task specific features can help 
   
   evid
*** table 2
brings up a number of interesting things
** how to fine-tune
** importance of the initial training task
** differences between transformers and LSTMs
** which layers of which models are most generalizable?

* DONE Recreate results8 to include RBF CKA.
  :PROPERTIES:
  :ARCHIVE_TIME: 2019-08-05 Mon 11:17
  :ARCHIVE_FILE: ~/core/mind/sys/short/proc/main.org
  :ARCHIVE_OLPATH: yonatan/contextual-corr-analysis
  :ARCHIVE_CATEGORY: main
  :ARCHIVE_TODO: DONE
  :END:
** DONE Run RBF CKA.
*** DONE Make script
*** DONE Run script
** DONE Receive results (or possible failure)
   
   OK, the gpu failed. What the fuck. This error message
           cat mk_results8-rbfcka1.out 
     7it [09:02, 77.36s/it]
     Loading representations

     Initializing methods ['rbfcka']

     Computing correlations
     For method:  rbfcka
     rbfcka:   0%|          | 0/8100 [00:00<?, ?it/s]Traceback (most recent call last):
       File "../main.py", line 120, in <module>
         disable_cuda=args.disable_cuda) 
       File "../main.py", line 100, in main
         method.compute_correlations()
       File "/data/sls/u/urop/johnmwu/contextual-corr-analysis/corr_methods.py", line 571, in compute_correlations
         Gx = center_gram(gram_rbf(X))
       File "/data/sls/u/urop/johnmwu/contextual-corr-analysis/corr_methods.py", line 538, in gram_rbf
         dot_products = torch.mm(X, X.t())
     RuntimeError: cublas runtime error : the GPU program failed to execute at /opt/conda/conda-bld/pytorch_1556653215914/work/aten/src/THC/THCBlas.cu:259
** DONE Try again with cpu machines. 
   
   Running on 630 machines rn.
** DONE Somehow, the results are incorrect. That's a bit
   worrying. Need to make them correct again.
   
   One interesting thing is that all columns are the same value.
   
   Another interesting thing is that mk_results9 is correct. Is it an
   error in the code? That'll be answered by another run of
   mk_results9.
*** DONE Rerun mk_results9, see if I get the same thing
     
     The rerun on the gpu failed. That's worrying, but not the
     problem we're trying to solve. Trying again on the trusted 630
     machines..
     
     Ok, there seem to be no errors at least..
*** DONE Make another analysis nb, see if the results are
     correct. No.
*** DONE What's wrong in the code? Ahh, I see. Let's try 9 again.
*** DONE Regen the second analysis-9 nb.
** DONE Rerun for results8
** DONE Receive results
** DONE Add results of rbfcka to analysis-8

* DONE Create feature to use the full (stacked) representation of the
  :PROPERTIES:
  :ARCHIVE_TIME: 2019-08-05 Mon 11:17
  :ARCHIVE_FILE: ~/core/mind/sys/short/proc/main.org
  :ARCHIVE_OLPATH: yonatan/contextual-corr-analysis
  :ARCHIVE_CATEGORY: main
  :ARCHIVE_TODO: DONE
  :END:
 model.
** DONE Make the ability to parse the string "full" from the optfile
*** DONE Change hnb
*** DONE Test
*** DONE Copy over
** DONE Make load_representations able to load it
*** DONE Edit hnb
*** DONE Copy over
** DONE Commit

* DONE Make heatmaps aligned.
  :PROPERTIES:
  :ARCHIVE_TIME: 2019-08-05 Mon 11:17
  :ARCHIVE_FILE: ~/core/mind/sys/short/proc/main.org
  :ARCHIVE_OLPATH: yonatan/contextual-corr-analysis
  :ARCHIVE_CATEGORY: main
  :ARCHIVE_TODO: DONE
  :END:

* DONE Create results8 nb. Probs just a copy of results7.
  :PROPERTIES:
  :ARCHIVE_TIME: 2019-08-05 Mon 11:17
  :ARCHIVE_FILE: ~/core/mind/sys/short/proc/main.org
  :ARCHIVE_OLPATH: yonatan/contextual-corr-analysis
  :ARCHIVE_CATEGORY: main
  :ARCHIVE_TODO: DONE
  :END:

* DONE Reorganize root dir of project
  :PROPERTIES:
  :ARCHIVE_TIME: 2019-08-05 Mon 11:17
  :ARCHIVE_FILE: ~/core/mind/sys/short/proc/main.org
  :ARCHIVE_OLPATH: yonatan/contextual-corr-analysis
  :ARCHIVE_CATEGORY: main
  :ARCHIVE_TODO: DONE
  :END:

* DONE Heatmap for similarity measures would be cool
  :PROPERTIES:
  :ARCHIVE_TIME: 2019-08-05 Mon 11:17
  :ARCHIVE_FILE: ~/core/mind/sys/short/proc/main.org
  :ARCHIVE_OLPATH: yonatan/contextual-corr-analysis
  :ARCHIVE_CATEGORY: main
  :ARCHIVE_TODO: DONE
  :END:
** DONE Try seaborn
** DONE

* DONE Determine mem usage of rbf cka? (w)
  :PROPERTIES:
  :ARCHIVE_TIME: 2019-08-05 Mon 11:17
  :ARCHIVE_FILE: ~/core/mind/sys/short/proc/main.org
  :ARCHIVE_OLPATH: yonatan/contextual-corr-analysis
  :ARCHIVE_CATEGORY: main
  :ARCHIVE_TODO: DONE
  :END:
 
 Emailed Yonatan. See what he says.

* DONE Read calypso, and openai transformer
  CLOSED: [2019-08-05 Mon 14:04]
  :PROPERTIES:
  :ARCHIVE_TIME: 2019-08-05 Mon 14:04
  :ARCHIVE_FILE: ~/core/mind/sys/short/proc/main.org
  :ARCHIVE_OLPATH: yonatan/contextual-corr-analysis
  :ARCHIVE_CATEGORY: main
  :ARCHIVE_TODO: DONE
  :END:
  - [X] openai transformer
    - [X] Wait. Does elmo do fine tuning?
      
      They say no. It doesn't. But, how to include elmo seems to
      differ among end tasks.
    - [X] Define generative model
    - [X] Is there a GLUE leaderboard history?
      
      Appears not.. also GPT-1 is not on the GLUE leaderboard. It's
      kind of strange, the GLUE leaderboard doesn't have that many
      entries.
    
    main points
    - unsupervised learning is extremely important
    - input transformations for specific tasks
    - fine tuned using a lc of downstream obj and lm objective
    - hypothesize that the underlying generative model learns to
      perform mand of the tasks evaluated on
    - they have a table where they display the importance of pretraining
  - [X] gpt-2
    
    main points
    - Use new dataset called WebText
    - LM can perform downstream tasks in a zero-shot
      setting. Actually, that's the main thing in this paper: zero
      shot learning.
    - Hmm.. thesis seems to be: if you can do language modelling, you
      can do anything.
    - Supports the thesis that bigger is better
    - Large language models are very slow in learning
    - Scrape all outbound links with at least 3 karma on Reddit, total
      40GB of text
      
    comment
    - This paper could also be called "Language modelling is all you
      need"
      
    question
    
    task
    - [X] Enter into anki
    - [X] 
  - [X] calypso
    
    Hmm.. seems like there isn't a paper. 

* DONE Decide on what to do for PWCCA
     CLOSED: [2019-08-05 Mon 17:11]
     CLOCK: [2019-08-05 Mon 14:22]--[2019-08-05 Mon 14:22] =>  0:00
  :PROPERTIES:
  :ARCHIVE_TIME: 2019-08-05 Mon 19:03
  :ARCHIVE_FILE: ~/core/mind/sys/short/proc/main.org
  :ARCHIVE_OLPATH: yonatan/contextual-corr-analysis/Top k correlations in SVCCA?
  :ARCHIVE_CATEGORY: main
  :ARCHIVE_TODO: DONE
  :END:
One observation is that taking the sum or the mean makes no difference.

Ahh, ok. I think I can save
- alignments (raw)
- alignments * corrs
** DONE Edit ju notebook to do these extra computations
   CLOSED: [2019-08-05 Mon 16:46]
   CLOCK: [2019-08-05 Mon 14:22]--[2019-08-05 Mon 16:46] =>  2:24
** DONE Copy paste the notebook in
   CLOSED: [2019-08-05 Mon 17:09]
   CLOCK: [2019-08-05 Mon 16:46]--[2019-08-05 Mon 17:09] =>  0:23
** DONE Edit how writing works
   CLOSED: [2019-08-05 Mon 17:11]
   CLOCK: [2019-08-05 Mon 17:09]--[2019-08-05 Mon 17:11] =>  0:02

* DONE Edit the jupyter notebook to add this info (just svcca for now)
  CLOSED: [2019-08-05 Mon 19:03]
  :PROPERTIES:
  :ARCHIVE_TIME: 2019-08-05 Mon 19:03
  :ARCHIVE_FILE: ~/core/mind/sys/short/proc/main.org
  :ARCHIVE_OLPATH: yonatan/contextual-corr-analysis/Top k correlations in SVCCA?
  :ARCHIVE_CATEGORY: main
  :ARCHIVE_TODO: DONE
  :END:
** DONE Can I get a scrolling, interactive heatmap?
      CLOSED: [2019-08-05 Mon 19:03]
      CLOCK: [2019-08-05 Mon 17:51]--[2019-08-05 Mon 19:03] =>  1:12
OK, so it is possible. I should look into a few things:
- [X] ipywidgets
  - [X] What is a "widget"?
  - [X] Try getting a heatmap to display

* TODO Change the casing on if network == other_network
     CLOCK: [2019-08-06 Tue 12:03]--[2019-08-06 Tue 12:45] =>  0:42
  :PROPERTIES:
  :ARCHIVE_TIME: 2019-08-06 Tue 12:43
  :ARCHIVE_FILE: ~/core/mind/sys/short/proc/main.org
  :ARCHIVE_OLPATH: yonatan/contextual-corr-analysis/Top k correlations in SVCCA?
  :ARCHIVE_CATEGORY: main
  :ARCHIVE_TODO: TODO
  :END:
NVM, just leave it as a continue. I think the issue is because the
singular values are not different from one another.
** DONE Create a place for computer errors in my fs
      CLOSED: [2019-08-06 Tue 12:43]

- [X] How to automatically refile to this location?

* DONE For linreg, create a similarity dict. Forgot to do this. 
  CLOSED: [2019-08-06 Tue 19:30]
  CLOCK: [2019-08-06 Tue 19:25]--[2019-08-06 Tue 19:30] =>  0:05
  :PROPERTIES:
  :ARCHIVE_TIME: 2019-08-06 Tue 19:40
  :ARCHIVE_FILE: ~/core/mind/sys/short/proc/main.org
  :ARCHIVE_OLPATH: yonatan/contextual-corr-analysis/Create correlations measures out of corrs and linreg.
  :ARCHIVE_CATEGORY: main
  :ARCHIVE_TODO: DONE
  :END:
  

* DONE corr
     CLOSED: [2019-08-06 Tue 19:17]
     CLOCK: [2019-08-06 Tue 14:42]--[2019-08-06 Tue 19:17] =>  4:35
  :PROPERTIES:
  :ARCHIVE_TIME: 2019-08-06 Tue 19:40
  :ARCHIVE_FILE: ~/core/mind/sys/short/proc/main.org
  :ARCHIVE_OLPATH: yonatan/contextual-corr-analysis/Create correlations measures out of corrs and linreg.
  :ARCHIVE_CATEGORY: main
  :ARCHIVE_TODO: DONE
  :END:
Hmm... =neuron_notated_sort= doesn't seem to be the correct data
structure.

What should it be?

{network: [(neuron, {other: val})]} just like linreg.

Thus, we need to get rid of self.clusters

- [X] Change write_correlations
- [X] Add similarity functionality

* DONE linreg
     CLOSED: [2019-08-06 Tue 14:38]
     CLOCK: [2019-08-06 Tue 13:47]--[2019-08-06 Tue 14:38] =>  0:51
  :PROPERTIES:
  :ARCHIVE_TIME: 2019-08-06 Tue 19:40
  :ARCHIVE_FILE: ~/core/mind/sys/short/proc/main.org
  :ARCHIVE_OLPATH: yonatan/contextual-corr-analysis/Create correlations measures out of corrs and linreg.
  :ARCHIVE_CATEGORY: main
  :ARCHIVE_TODO: DONE
  :END:
So actually, the information is all computed already, I would just need
to write it to a file.

I need to change =write_correlations=.

- [X] Make it save the neuron_sort, neuron_notated_sort, and the
  pred_power
  - [X] Edit compute correlations hnb
  - [X] Copy stuff over to .py file
  - [X] Edit writing; I think it should use =torch.save= now..
  - [X] Copy stuff over to .py file

* TODO Edit analysis nb
  :PROPERTIES:
  :ARCHIVE_TIME: 2019-08-06 Tue 19:41
  :ARCHIVE_FILE: ~/core/mind/sys/short/proc/main.org
  :ARCHIVE_OLPATH: yonatan/contextual-corr-analysis/Create correlations measures out of corrs and linreg.
  :ARCHIVE_CATEGORY: main
  :ARCHIVE_TODO: TODO
  :END:
- [ ] Change loading, because the formats of the files are different now
- [ ] Allow using the correlations as similarity measures     
- [ ] 

This can wait.   

* TODO Regenerate output files for analysis-8
  :PROPERTIES:
  :ARCHIVE_TIME: 2019-08-06 Tue 19:41
  :ARCHIVE_FILE: ~/core/mind/sys/short/proc/main.org
  :ARCHIVE_OLPATH: yonatan/contextual-corr-analysis/Create correlations measures out of corrs and linreg.
  :ARCHIVE_CATEGORY: main
  :ARCHIVE_TODO: TODO
  :END:

Actually, this can wait.      

* DONE Create correlations measures out of corrs and linreg. 
  CLOSED: [2019-08-06 Tue 19:41]
  CLOCK: [2019-08-06 Tue 13:45]--[2019-08-06 Tue 13:47] =>  0:02
  :PROPERTIES:
  :ARCHIVE_TIME: 2019-08-06 Tue 19:41
  :ARCHIVE_FILE: ~/core/mind/sys/short/proc/main.org
  :ARCHIVE_OLPATH: yonatan/contextual-corr-analysis
  :ARCHIVE_CATEGORY: main
  :ARCHIVE_TODO: DONE
  :END:
 
 You could aggregate values in various ways -- mean, max. 
 
 There are more ways to use cca. For example, take the average of the
 top k cc values.

 Note: don't skip the pair between a layer and itself, because these
 won't be 1.

* DONE Prepare for meeting tomorrow morning
  CLOSED: [2019-08-06 Tue 19:41]
  :PROPERTIES:
  :ARCHIVE_TIME: 2019-08-06 Tue 19:41
  :ARCHIVE_FILE: ~/core/mind/sys/short/proc/main.org
  :ARCHIVE_OLPATH: yonatan/contextual-corr-analysis
  :ARCHIVE_CATEGORY: main
  :ARCHIVE_TODO: DONE
  :END:

* TODO Change everything to use pickling
  :PROPERTIES:
  :ARCHIVE_TIME: 2019-08-07 Wed 11:39
  :ARCHIVE_FILE: ~/core/mind/sys/short/proc/main.org
  :ARCHIVE_OLPATH: yonatan/contextual-corr-analysis
  :ARCHIVE_CATEGORY: main
  :ARCHIVE_TODO: TODO
  :END:
Will need to make everything become NumPy I think.     

* DONE Change everything to use pickling
    CLOSED: [2019-08-07 Wed 11:49]
    CLOCK: [2019-08-07 Wed 11:39]--[2019-08-07 Wed 11:49] =>  0:10
  :PROPERTIES:
  :ARCHIVE_TIME: 2019-08-07 Wed 11:49
  :ARCHIVE_FILE: ~/core/mind/sys/short/proc/main.org
  :ARCHIVE_OLPATH: yonatan/contextual-corr-analysis
  :ARCHIVE_CATEGORY: main
  :ARCHIVE_TODO: DONE
  :END:
Will need to make everything become NumPy I think.

Wait. When should I make it numpy? Perhaps just at the end.. actually,
it's not super important...

- [X] maxmin
- [X] linreg
- [X] cca
- [X] lincka
- [X] rbfcka

* TODO Have a look at results from hassan and nadir
  CLOCK: [2019-08-07 Wed 11:54]--[2019-08-07 Wed 12:34] =>  0:40
  :PROPERTIES:
  :ARCHIVE_TIME: 2019-08-07 Wed 13:49
  :ARCHIVE_FILE: ~/core/mind/sys/short/proc/main.org
  :ARCHIVE_OLPATH: yonatan/contextual-corr-analysis
  :ARCHIVE_CATEGORY: main
  :ARCHIVE_TODO: TODO
  :END:

* DONE Commit revised cca
    CLOSED: [2019-08-07 Wed 14:06]
  :PROPERTIES:
  :ARCHIVE_TIME: 2019-08-07 Wed 14:06
  :ARCHIVE_FILE: ~/core/mind/sys/short/proc/main.org
  :ARCHIVE_OLPATH: yonatan/contextual-corr-analysis
  :ARCHIVE_CATEGORY: main
  :ARCHIVE_TODO: DONE
  :END:
- [X] Overwrite previous commit    

* DONE if network==other_network in maxmin and linreg
    CLOSED: [2019-08-08 Thu 12:34]
  :PROPERTIES:
  :ARCHIVE_TIME: 2019-08-08 Thu 12:37
  :ARCHIVE_FILE: ~/core/mind/sys/short/proc/main.org
  :ARCHIVE_OLPATH: yonatan/contextual-corr-analysis
  :ARCHIVE_CATEGORY: main
  :ARCHIVE_TODO: DONE
  :END:
Consensus is that this is unnecessary. My email to Yonatan:

Well, maxmincorr is extracted from the correlation matrix, so it
contains strictly more information than just the index and correlation
of the most correlated neuron pair.

My point is that I don't think we should remove the line

            if network == other_network:
                continue

In MaxMinCorr. The reasoning is:
- If we did, I don't think it would tell us much. All the info we'd have
  would be pairs. The whole correlation matrix is strictly better.
- If we did and added the whole correlation matrix to the output for
  every pair, our output would be way too big, and most of it would be
  unused.
 
Thus, I feel like this would be better to add as a different method,
probably in a completely different script. Even semantically, the
=corr_methods.py= module is supposed to be comparing representations
across layers, not intra-layer representations. Also, intra-layer
representations will require completely different methods, because the
goal is no longer determining correlation.

You also mentioned doing this for LinReg. What exactly did you mean? I'm
not sure how to interpret regressing the rest of the layer onto a
particular neuron means. I feel like the interpretation would be the
opposite of the others---the higher the predictive power, the more
useless the neuron is, because it just means that it's direction can
already be expressed as a linear combination of others. This isn't
precise because the learning dynamics affects this, but again, I feel
like it has to be treated differently.

Also, it might screw up the neuron sorts. 

* DONE Reorganize the =slurm= directory
    CLOSED: [2019-08-08 Thu 15:47]
  :PROPERTIES:
  :ARCHIVE_TIME: 2019-08-08 Thu 15:47
  :ARCHIVE_FILE: ~/core/mind/sys/short/proc/main.org
  :ARCHIVE_OLPATH: yonatan/contextual-corr-analysis
  :ARCHIVE_CATEGORY: main
  :ARCHIVE_TODO: DONE
  :END:
- [X] Change 8 and 11 to use the mk_results1 style    

* TODO Why did minlinreg fail?
     CLOCK: [2019-08-08 Thu 18:49]--[2019-08-09 Fri 12:04] => 17:15
  :PROPERTIES:
  :ARCHIVE_TIME: 2019-08-08 Thu 18:56
  :ARCHIVE_FILE: ~/core/mind/sys/short/proc/main.org
  :ARCHIVE_OLPATH: yonatan/contextual-corr-analysis/Top k correlations in SVCCA?
  :ARCHIVE_CATEGORY: main
  :ARCHIVE_TODO: TODO
  :END:
Wait, I don't think it did.      

* DONE Rewrite maxmin and meanstd computations
     CLOSED: [2019-08-09 Fri 19:05]
     CLOCK: [2019-08-09 Fri 18:40]--[2019-08-09 Fri 19:05] =>  0:25
     CLOCK: [2019-08-08 Thu 17:24]--[2019-08-08 Thu 18:49] =>  1:25
  :PROPERTIES:
  :ARCHIVE_TIME: 2019-08-09 Fri 19:06
  :ARCHIVE_FILE: ~/core/mind/sys/short/proc/main.org
  :ARCHIVE_OLPATH: yonatan/contextual-corr-analysis/Visualizations: cmaps, more sentences, more neurons
  :ARCHIVE_CATEGORY: main
  :ARCHIVE_TODO: DONE
  :END:
I feel like many of these computations can be rewritten so they read
from files on disk, and operate on files on disk. I feel like this is
particularly true of simple statistics like mean, std, etc.

So, what should the format of such a file be?

{
    "mean": {model : means},
    "std" : {model : std},
    "max" : {model : max},
    "min" : {model : min},
}

Ahh, shit.

Waiting on jobid 7691. Great. Works?

Great. Waiting on jobid 7694. 

* DONE Convert all computers to ubuntu 18
   CLOSED: [2019-08-09 Fri 19:50]
   CLOCK: [2019-08-09 Fri 19:06]--[2019-08-09 Fri 19:50] =>  0:44
   CLOCK: [2019-08-09 Fri 18:03]--[2019-08-09 Fri 18:04] =>  0:01
   CLOCK: [2019-08-09 Fri 18:03]--[2019-08-09 Fri 18:03] =>  0:00
   CLOCK: [2019-08-09 Fri 18:03]--[2019-08-09 Fri 18:03] =>  0:00
   CLOCK: [2019-08-09 Fri 17:51]--[2019-08-09 Fri 18:03] =>  0:12
   CLOCK: [2019-08-09 Fri 15:34]--[2019-08-09 Fri 15:54] =>  0:20
   CLOCK: [2019-08-09 Fri 14:53]--[2019-08-09 Fri 15:16] =>  0:23
   CLOCK: [2019-08-09 Fri 12:04]--[2019-08-09 Fri 12:44] =>  0:40
  :PROPERTIES:
  :ARCHIVE_TIME: 2019-08-09 Fri 19:50
  :ARCHIVE_FILE: ~/core/mind/sys/short/proc/main.org
  :ARCHIVE_OLPATH: yonatan
  :ARCHIVE_CATEGORY: main
  :ARCHIVE_TODO: DONE
  :END:
Waiting on Jim to see if I can be added to =sls-sudoers=.

Waiting on installers on quad, beorn, and sugar to finish.

- [X] begin installer on chris's computer
Wait for that to finish
- [X] Login on all machines, make sure that /data/sls is accessible. 

* DONE [#A] Top k correlations in SVCCA?
    CLOSED: [2019-08-12 Mon 13:42]
    CLOCK: [2019-08-05 Mon 14:05]--[2019-08-05 Mon 14:22] =>  0:17
  :PROPERTIES:
  :ARCHIVE_TIME: 2019-08-12 Mon 13:42
  :ARCHIVE_FILE: ~/core/mind/sys/short/proc/main.org
  :ARCHIVE_OLPATH: yonatan/contextual-corr-analysis
  :ARCHIVE_CATEGORY: main
  :ARCHIVE_TODO: DONE
  :END:

I don't think it's useful to put code for it in =corr_methods.py=. I
should just load the canonical correlations, and do everything directly.

There is also an interesting idea: could I get the heatmap to change, as
you change the value of k?

Well, for PWCCA, this is not as straightforward. It would require more
computation. Also, how would you even do it?
** DONE Rerun for svcca
     CLOSED: [2019-08-06 Tue 12:02]
Running, as of Mon Aug  5 19:23:29 EDT 2019
** DONE Rerun for svcca
     CLOSED: [2019-08-07 Wed 13:27]
In progress
** DONE Fix pwcca stuff
     CLOSED: [2019-08-08 Thu 12:24]
     CLOCK: [2019-08-08 Thu 12:24]--[2019-08-08 Thu 12:24] =>  0:00
- [X] Create a test script     
- [X] Run the test script
- [X] Get results
- [X] Rerun
- [X] Get results
** DONE Regen nb for analysis-8
     CLOSED: [2019-08-09 Fri 18:39]
     CLOCK: [2019-08-09 Fri 18:04]--[2019-08-09 Fri 18:29] =>  0:25
     CLOCK: [2019-08-09 Fri 17:22]--[2019-08-09 Fri 17:51] =>  0:29
     CLOCK: [2019-08-09 Fri 15:56]--[2019-08-09 Fri 16:02] =>  0:06
     CLOCK: [2019-08-09 Fri 12:44]--[2019-08-09 Fri 14:53] =>  2:09
     CLOCK: [2019-08-08 Thu 12:24]--[2019-08-08 Thu 12:49] =>  0:25
Actually, I should regen analysis-8 entirely.
- [X] Run all methods again
- [X] Get results
  
Ahh, shit. minlinreg failed. Actually, I don't think it failed.

waiting on jobid 5275, which is maxlinreg. OK, everything looks good.
- [X] Regen
  - [X] Make test sbatchable
  - [X] Wait on results
    
    Wait, so is the sbatch finished? Yep
  - [X] Regen first for the test
    
    We need to get the analysis nb working again. 
    - [X] Create a generic =update= function that works for both pw and
      sv.
    - [X] Create interactive heatmaps for maxmincorr and linregcorr
      - [X] Reindex to make rows = columns
  - [X] Copy test over, regen
    
    Waiting on notebook analysis-8 to finish

* DONE CCA
      CLOSED: [2019-08-13 Tue 13:59]
      CLOCK: [2019-08-13 Tue 12:41]--[2019-08-13 Tue 13:59] =>  1:18
  :PROPERTIES:
  :ARCHIVE_TIME: 2019-08-13 Tue 13:59
  :ARCHIVE_FILE: ~/core/mind/sys/short/proc/main.org
  :ARCHIVE_OLPATH: yonatan/contextual-corr-analysis/Add the more models suggested by yonatan./Look at errors. Some errors may be due to memory; should try and fix these.
  :ARCHIVE_CATEGORY: main
  :ARCHIVE_TODO: DONE
  :END:
Shit... this may actually be coded wrong. Ahhhhhhh shit.

Okay, well would it be good now? What is taking up memory? Yeah, should
be good now. If I give 240GB, it should be all g. 


* DONE [#A] Evidence that RBFCKA is stable under different sizes of the dataset
    CLOSED: [2019-08-14 Wed 11:25]
    CLOCK: [2019-08-09 Fri 16:02]--[2019-08-09 Fri 17:20] =>  1:18
  :PROPERTIES:
  :ARCHIVE_TIME: 2019-08-14 Wed 11:25
  :ARCHIVE_FILE: ~/core/mind/sys/short/proc/main.org
  :ARCHIVE_OLPATH: yonatan/contextual-corr-analysis
  :ARCHIVE_CATEGORY: main
  :ARCHIVE_TODO: DONE
  :END:
- [X] How exactly would I get such evidence?
  
  My code currently supports taking the first k words of the corpus. I
  could run this with different values of k. However, the initial values
  would then be overloaded. I don't think this is a huge deal,
  though. Alternatively, I could take samples of size 10000. 
  
  I think what I'll do is just 5000, 10000, 15000, 20000. I don't think
  the evidence needs to be super statistically motivated; as long as we
  say we didn't see much of a difference, I think it's ok.
- [X] Setup everything for running =mk_results12.sh=. 
  - [X] Create the script, and dependencies
    - [X] Create mk_results12-helper.sh
    - [X] 
  - [X] Alter readme
- [X] Run mk_results12.sh
- [X] Get results
  
  Waiting on jobs 1098{4..7}
- [X] Why did it fail? Wait, what? time limit exceeded? Fuck...
- [X] How many iterations did it get through?
- [X] Shit.. Actually though, why is it even using dask? 
  - [X] Before that, change the limit to affect the original tensors
    being loaded in, NOT just during RBFCKA. 
    - [X] Remove the limit stuff from RBFCKA
    - [X] Add limit stuff in loading
      - [X] I should raise an error in the loading if the results are
        not all the same shape. Actually, nvm. That can be implemented
        as a separate function, =check_correct= or something. 
      - [X] sldkfj
    - [X] 
  - [X] I'll test again, we'll see
    
    11620       gpu mk_resul  johnmwu  R       0:30      1 sls-sm-6
    11621       gpu mk_resul  johnmwu  R       0:30      1 sls-sm-3
    11622       gpu mk_resul  johnmwu  R       0:30      1 sls-sm-3
    11623       gpu mk_resul  johnmwu  R       0:30      1 sls-titan-10
  - [X] Try again with some print statements. This is really weird that
    only 20000 doesn't work.
    
    Hmm... idk.. it seems to have taken 5h.
** DONE Create results12 with different filenames. fuck!
     CLOSED: [2019-08-14 Wed 10:56]
     CLOCK: [2019-08-14 Wed 10:54]--[2019-08-14 Wed 10:56] =>  0:02
Waiting on jobs 3049{2,3,4}     
** DONE Create an analysis notebook for RBFCKA. 
   CLOSED: [2019-08-14 Wed 11:25]
   CLOCK: [2019-08-14 Wed 10:56]--[2019-08-14 Wed 11:25] =>  0:29

* DONE [#A] Correlations of random activations
    CLOSED: [2019-08-14 Wed 13:42]
    CLOCK: [2019-08-14 Wed 13:23]--[2019-08-14 Wed 13:42] =>  0:19
    CLOCK: [2019-08-13 Tue 18:35]--[2019-08-13 Tue 18:43] =>  0:08
    CLOCK: [2019-08-13 Tue 14:05]--[2019-08-13 Tue 14:21] =>  0:16
  :PROPERTIES:
  :ARCHIVE_TIME: 2019-08-14 Wed 13:42
  :ARCHIVE_FILE: ~/core/mind/sys/short/proc/main.org
  :ARCHIVE_OLPATH: yonatan/contextual-corr-analysis
  :ARCHIVE_CATEGORY: main
  :ARCHIVE_TODO: DONE
  :END:

- [X] Create script     
  - [X] Create helper
    - [X] Create opt13 and repr_files13
  - [X] 
- [X] Edit readme
- [X] Get results
  
  Submitted batch job 30751
  Submitted batch job 30752
  Submitted batch job 30753
  Submitted batch job 30754
  Submitted batch job 30755
  Submitted batch job 30756
OK, I can now copy over the analysis nb. Ahh, shit! I fucked it up.

I need to somehow make =representations_d= able to use indices that are
other than just the basename of the dirname. The simplest solution is
just to change fname2mname.

Fuck. I'd thought that the structure would be more something like
=/elmo_rand/ptb_pos_dev.hdf5=
=/elmo/ptb_pos_dev.hdf5=
than what it currently is.

Okee, let's continue
- [X] Get results
  
  Submitted batch job 32135
  Submitted batch job 32136
  Submitted batch job 32137
  Submitted batch job 32138
  Submitted batch job 32139
  Submitted batch job 32140
- [X] Create analysis13 nb?


* DONE For RBFCKA, compute some statistics
  CLOSED: [2019-08-14 Wed 14:07]
  CLOCK: [2019-08-14 Wed 13:48]--[2019-08-14 Wed 14:07] =>  0:19
  :PROPERTIES:
  :ARCHIVE_TIME: 2019-08-14 Wed 14:07
  :ARCHIVE_FILE: ~/core/mind/sys/short/proc/main.org
  :ARCHIVE_OLPATH: yonatan/contextual-corr-analysis
  :ARCHIVE_CATEGORY: main
  :ARCHIVE_TODO: DONE
  :END:

  
