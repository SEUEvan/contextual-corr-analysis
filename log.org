
Archived entries from file /afs/csail.mit.edu/u/j/johnmwu/core/mind/sys/short/proc/main.org


* DONE Universality and individuality in neural dynamics
  :PROPERTIES:
  :ARCHIVE_TIME: 2019-08-05 Mon 11:16
  :ARCHIVE_FILE: ~/core/mind/sys/short/proc/main.org
  :ARCHIVE_OLPATH: yonatan/contextual-corr-analysis
  :ARCHIVE_CATEGORY: main
  :ARCHIVE_TODO: DONE
  :END:
  on sec 3.3
** DONE Define double centering
** DONE Define MDS
** DONE Summarize
*** CCA based similarity can be misleading, and can depend on the
      activation function.
*** CCA based similarity will cluster based on architecture and
      activation, but the "fixed point topology" and linearized
      dynamics remains similar.
** DONE email them about typo on pg 8
** DONE Are these three tasks standard?
*** K bit flip flop uhh doesn't seem to be
*** sine wave lol... this also comes from some paper written
      earlier by one of the authors
*** CDI doesn't seem to be
** DONE How exactly do they construct this "fixed point topology
    MDS"?
    
    I see. It seems:
*** Construct a markov chain consisting of the fixed points for
      each model
*** Construct a connectivity matrix for the fixed point graph
*** Put all these matrices in the same space, by imposing some
      order on the set of fixed points
      
      Throw out all models which have a non-modal number of fixed
      points.
*** Use euclidean distance (I guess the frobenius norm) to
      quantify dissimilarity.
** DONE How is this constructed for sine wave generation?
    
    Hmm... they don't seem to mention this.
** DONE Add comments to the paper pdf file
** DONE Add into anki
comment
** I feel like their topology analysis is interesting, but sort of
    just confirms the obvious. Yes, all the models are trained on
    the same task, so of course they're learning the same things.
** It would be helpful if they released code. The following things
    are not made clear:
*** How they do fixed point topology MDS (slightly more clear on
      the flip flop task, but even there not exactly)
*** What they mean by eigvals (fig 3e) is the eigvals of some
      section of the jacobian matrix at a fixed point, right?

* DONE I need to learn the standard terminology for all of the nlp
  :PROPERTIES:
  :ARCHIVE_TIME: 2019-08-05 Mon 11:17
  :ARCHIVE_FILE: ~/core/mind/sys/short/proc/main.org
  :ARCHIVE_OLPATH: yonatan/contextual-corr-analysis
  :ARCHIVE_CATEGORY: main
  :ARCHIVE_TODO: DONE
  :END:
 tasks
 
 task
** DONE part of speech tagging
*** DONE Read the initial ptb paper
     
     comment
**** Wait. So do they present the tagset?
**** Interesting.. it seems that papers presenting a dataset go
       into a lot of detail about how the data was collected. It
       makes sense.
**** They talk about the "training of annotators". Isn't this kind
       of like training an organic neural network on a small amt of
       data, to then generate more data for another neural network to
       train on?
**** They talk about annotators having 3 or 4 months of
       experience.. wtf? How long were annotators working on this
       damn thing?
**** This paper renders trees as sexps. Respect.
task
**** DONE Add into anki
**** DONE
*** DONE
** DONE Chunking
*** DONE Read the conll-2000 shared task paper
*** DONE Enter "Chunking" into anki
** DONE named entity recognition
*** DONE Read the conll-2003 paper
** DONE natural language inference
*** DONE Read the SNLI paper
**** DONE OK... what is GloVe?
       
       Oh shit. It'd be cool to have a look at these embeddings.
       
       task
***** DONE Try looking at some GloVe vectors?
****** DONE Get jupyter notebook working from within emacs. Ahh
           fuck. It needs 25.3, which ubuntu doesn't have.
****** DONE Move the misc notebook into my filesystem
***** DONE Add into anki
**** DONE
*** DONE
** DONE coreference resolution
   
   task
*** DONE Read conll 2012
     
     Interesting
**** chinese word segmentation is fairly accurate (pg 9)
task
**** DONE Wait so what are these shared tasks? Ok... so it seems
       that each year there is a shared task.
*** DONE Define "coreference resolution"
** DONE

* DONE Read Yonatan's paper with Nelson
  :PROPERTIES:
  :ARCHIVE_TIME: 2019-08-05 Mon 11:17
  :ARCHIVE_FILE: ~/core/mind/sys/short/proc/main.org
  :ARCHIVE_OLPATH: yonatan/contextual-corr-analysis
  :ARCHIVE_CATEGORY: main
  :ARCHIVE_TODO: DONE
  :END:
 
 What do they mean by "transferable"?
 
 What do they mean by "task trained" LSTM in section 4.2?
 
 comment
** the list of tasks should probably be put in an appendix.
** It's hard to exactly say the main points of this paper..
main points
** language modeling is the most transferable task
   
   evid
*** sec 6.2 and table 3
** lower layers of LSTMs are more transferable, but this is not true
   of transformers
   
   evid
*** figure 3 and 4
** learning task specific features can help 
   
   evid
*** table 2
brings up a number of interesting things
** how to fine-tune
** importance of the initial training task
** differences between transformers and LSTMs
** which layers of which models are most generalizable?

* DONE Recreate results8 to include RBF CKA.
  :PROPERTIES:
  :ARCHIVE_TIME: 2019-08-05 Mon 11:17
  :ARCHIVE_FILE: ~/core/mind/sys/short/proc/main.org
  :ARCHIVE_OLPATH: yonatan/contextual-corr-analysis
  :ARCHIVE_CATEGORY: main
  :ARCHIVE_TODO: DONE
  :END:
** DONE Run RBF CKA.
*** DONE Make script
*** DONE Run script
** DONE Receive results (or possible failure)
   
   OK, the gpu failed. What the fuck. This error message
           cat mk_results8-rbfcka1.out 
     7it [09:02, 77.36s/it]
     Loading representations

     Initializing methods ['rbfcka']

     Computing correlations
     For method:  rbfcka
     rbfcka:   0%|          | 0/8100 [00:00<?, ?it/s]Traceback (most recent call last):
       File "../main.py", line 120, in <module>
         disable_cuda=args.disable_cuda) 
       File "../main.py", line 100, in main
         method.compute_correlations()
       File "/data/sls/u/urop/johnmwu/contextual-corr-analysis/corr_methods.py", line 571, in compute_correlations
         Gx = center_gram(gram_rbf(X))
       File "/data/sls/u/urop/johnmwu/contextual-corr-analysis/corr_methods.py", line 538, in gram_rbf
         dot_products = torch.mm(X, X.t())
     RuntimeError: cublas runtime error : the GPU program failed to execute at /opt/conda/conda-bld/pytorch_1556653215914/work/aten/src/THC/THCBlas.cu:259
** DONE Try again with cpu machines. 
   
   Running on 630 machines rn.
** DONE Somehow, the results are incorrect. That's a bit
   worrying. Need to make them correct again.
   
   One interesting thing is that all columns are the same value.
   
   Another interesting thing is that mk_results9 is correct. Is it an
   error in the code? That'll be answered by another run of
   mk_results9.
*** DONE Rerun mk_results9, see if I get the same thing
     
     The rerun on the gpu failed. That's worrying, but not the
     problem we're trying to solve. Trying again on the trusted 630
     machines..
     
     Ok, there seem to be no errors at least..
*** DONE Make another analysis nb, see if the results are
     correct. No.
*** DONE What's wrong in the code? Ahh, I see. Let's try 9 again.
*** DONE Regen the second analysis-9 nb.
** DONE Rerun for results8
** DONE Receive results
** DONE Add results of rbfcka to analysis-8

* DONE Create feature to use the full (stacked) representation of the
  :PROPERTIES:
  :ARCHIVE_TIME: 2019-08-05 Mon 11:17
  :ARCHIVE_FILE: ~/core/mind/sys/short/proc/main.org
  :ARCHIVE_OLPATH: yonatan/contextual-corr-analysis
  :ARCHIVE_CATEGORY: main
  :ARCHIVE_TODO: DONE
  :END:
 model.
** DONE Make the ability to parse the string "full" from the optfile
*** DONE Change hnb
*** DONE Test
*** DONE Copy over
** DONE Make load_representations able to load it
*** DONE Edit hnb
*** DONE Copy over
** DONE Commit

* DONE Make heatmaps aligned.
  :PROPERTIES:
  :ARCHIVE_TIME: 2019-08-05 Mon 11:17
  :ARCHIVE_FILE: ~/core/mind/sys/short/proc/main.org
  :ARCHIVE_OLPATH: yonatan/contextual-corr-analysis
  :ARCHIVE_CATEGORY: main
  :ARCHIVE_TODO: DONE
  :END:

* DONE Create results8 nb. Probs just a copy of results7.
  :PROPERTIES:
  :ARCHIVE_TIME: 2019-08-05 Mon 11:17
  :ARCHIVE_FILE: ~/core/mind/sys/short/proc/main.org
  :ARCHIVE_OLPATH: yonatan/contextual-corr-analysis
  :ARCHIVE_CATEGORY: main
  :ARCHIVE_TODO: DONE
  :END:

* DONE Reorganize root dir of project
  :PROPERTIES:
  :ARCHIVE_TIME: 2019-08-05 Mon 11:17
  :ARCHIVE_FILE: ~/core/mind/sys/short/proc/main.org
  :ARCHIVE_OLPATH: yonatan/contextual-corr-analysis
  :ARCHIVE_CATEGORY: main
  :ARCHIVE_TODO: DONE
  :END:

* DONE Heatmap for similarity measures would be cool
  :PROPERTIES:
  :ARCHIVE_TIME: 2019-08-05 Mon 11:17
  :ARCHIVE_FILE: ~/core/mind/sys/short/proc/main.org
  :ARCHIVE_OLPATH: yonatan/contextual-corr-analysis
  :ARCHIVE_CATEGORY: main
  :ARCHIVE_TODO: DONE
  :END:
** DONE Try seaborn
** DONE

* DONE Determine mem usage of rbf cka? (w)
  :PROPERTIES:
  :ARCHIVE_TIME: 2019-08-05 Mon 11:17
  :ARCHIVE_FILE: ~/core/mind/sys/short/proc/main.org
  :ARCHIVE_OLPATH: yonatan/contextual-corr-analysis
  :ARCHIVE_CATEGORY: main
  :ARCHIVE_TODO: DONE
  :END:
 
 Emailed Yonatan. See what he says.

* DONE Read calypso, and openai transformer
  CLOSED: [2019-08-05 Mon 14:04]
  :PROPERTIES:
  :ARCHIVE_TIME: 2019-08-05 Mon 14:04
  :ARCHIVE_FILE: ~/core/mind/sys/short/proc/main.org
  :ARCHIVE_OLPATH: yonatan/contextual-corr-analysis
  :ARCHIVE_CATEGORY: main
  :ARCHIVE_TODO: DONE
  :END:
  - [X] openai transformer
    - [X] Wait. Does elmo do fine tuning?
      
      They say no. It doesn't. But, how to include elmo seems to
      differ among end tasks.
    - [X] Define generative model
    - [X] Is there a GLUE leaderboard history?
      
      Appears not.. also GPT-1 is not on the GLUE leaderboard. It's
      kind of strange, the GLUE leaderboard doesn't have that many
      entries.
    
    main points
    - unsupervised learning is extremely important
    - input transformations for specific tasks
    - fine tuned using a lc of downstream obj and lm objective
    - hypothesize that the underlying generative model learns to
      perform mand of the tasks evaluated on
    - they have a table where they display the importance of pretraining
  - [X] gpt-2
    
    main points
    - Use new dataset called WebText
    - LM can perform downstream tasks in a zero-shot
      setting. Actually, that's the main thing in this paper: zero
      shot learning.
    - Hmm.. thesis seems to be: if you can do language modelling, you
      can do anything.
    - Supports the thesis that bigger is better
    - Large language models are very slow in learning
    - Scrape all outbound links with at least 3 karma on Reddit, total
      40GB of text
      
    comment
    - This paper could also be called "Language modelling is all you
      need"
      
    question
    
    task
    - [X] Enter into anki
    - [X] 
  - [X] calypso
    
    Hmm.. seems like there isn't a paper. 

* DONE Decide on what to do for PWCCA
     CLOSED: [2019-08-05 Mon 17:11]
     CLOCK: [2019-08-05 Mon 14:22]--[2019-08-05 Mon 14:22] =>  0:00
  :PROPERTIES:
  :ARCHIVE_TIME: 2019-08-05 Mon 19:03
  :ARCHIVE_FILE: ~/core/mind/sys/short/proc/main.org
  :ARCHIVE_OLPATH: yonatan/contextual-corr-analysis/Top k correlations in SVCCA?
  :ARCHIVE_CATEGORY: main
  :ARCHIVE_TODO: DONE
  :END:
One observation is that taking the sum or the mean makes no difference.

Ahh, ok. I think I can save
- alignments (raw)
- alignments * corrs
** DONE Edit ju notebook to do these extra computations
   CLOSED: [2019-08-05 Mon 16:46]
   CLOCK: [2019-08-05 Mon 14:22]--[2019-08-05 Mon 16:46] =>  2:24
** DONE Copy paste the notebook in
   CLOSED: [2019-08-05 Mon 17:09]
   CLOCK: [2019-08-05 Mon 16:46]--[2019-08-05 Mon 17:09] =>  0:23
** DONE Edit how writing works
   CLOSED: [2019-08-05 Mon 17:11]
   CLOCK: [2019-08-05 Mon 17:09]--[2019-08-05 Mon 17:11] =>  0:02

* DONE Edit the jupyter notebook to add this info (just svcca for now)
  CLOSED: [2019-08-05 Mon 19:03]
  :PROPERTIES:
  :ARCHIVE_TIME: 2019-08-05 Mon 19:03
  :ARCHIVE_FILE: ~/core/mind/sys/short/proc/main.org
  :ARCHIVE_OLPATH: yonatan/contextual-corr-analysis/Top k correlations in SVCCA?
  :ARCHIVE_CATEGORY: main
  :ARCHIVE_TODO: DONE
  :END:
** DONE Can I get a scrolling, interactive heatmap?
      CLOSED: [2019-08-05 Mon 19:03]
      CLOCK: [2019-08-05 Mon 17:51]--[2019-08-05 Mon 19:03] =>  1:12
OK, so it is possible. I should look into a few things:
- [X] ipywidgets
  - [X] What is a "widget"?
  - [X] Try getting a heatmap to display

* TODO Change the casing on if network == other_network
     CLOCK: [2019-08-06 Tue 12:03]--[2019-08-06 Tue 12:45] =>  0:42
  :PROPERTIES:
  :ARCHIVE_TIME: 2019-08-06 Tue 12:43
  :ARCHIVE_FILE: ~/core/mind/sys/short/proc/main.org
  :ARCHIVE_OLPATH: yonatan/contextual-corr-analysis/Top k correlations in SVCCA?
  :ARCHIVE_CATEGORY: main
  :ARCHIVE_TODO: TODO
  :END:
NVM, just leave it as a continue. I think the issue is because the
singular values are not different from one another.
** DONE Create a place for computer errors in my fs
      CLOSED: [2019-08-06 Tue 12:43]

- [X] How to automatically refile to this location?

* DONE For linreg, create a similarity dict. Forgot to do this. 
  CLOSED: [2019-08-06 Tue 19:30]
  CLOCK: [2019-08-06 Tue 19:25]--[2019-08-06 Tue 19:30] =>  0:05
  :PROPERTIES:
  :ARCHIVE_TIME: 2019-08-06 Tue 19:40
  :ARCHIVE_FILE: ~/core/mind/sys/short/proc/main.org
  :ARCHIVE_OLPATH: yonatan/contextual-corr-analysis/Create correlations measures out of corrs and linreg.
  :ARCHIVE_CATEGORY: main
  :ARCHIVE_TODO: DONE
  :END:
  

* DONE corr
     CLOSED: [2019-08-06 Tue 19:17]
     CLOCK: [2019-08-06 Tue 14:42]--[2019-08-06 Tue 19:17] =>  4:35
  :PROPERTIES:
  :ARCHIVE_TIME: 2019-08-06 Tue 19:40
  :ARCHIVE_FILE: ~/core/mind/sys/short/proc/main.org
  :ARCHIVE_OLPATH: yonatan/contextual-corr-analysis/Create correlations measures out of corrs and linreg.
  :ARCHIVE_CATEGORY: main
  :ARCHIVE_TODO: DONE
  :END:
Hmm... =neuron_notated_sort= doesn't seem to be the correct data
structure.

What should it be?

{network: [(neuron, {other: val})]} just like linreg.

Thus, we need to get rid of self.clusters

- [X] Change write_correlations
- [X] Add similarity functionality

* DONE linreg
     CLOSED: [2019-08-06 Tue 14:38]
     CLOCK: [2019-08-06 Tue 13:47]--[2019-08-06 Tue 14:38] =>  0:51
  :PROPERTIES:
  :ARCHIVE_TIME: 2019-08-06 Tue 19:40
  :ARCHIVE_FILE: ~/core/mind/sys/short/proc/main.org
  :ARCHIVE_OLPATH: yonatan/contextual-corr-analysis/Create correlations measures out of corrs and linreg.
  :ARCHIVE_CATEGORY: main
  :ARCHIVE_TODO: DONE
  :END:
So actually, the information is all computed already, I would just need
to write it to a file.

I need to change =write_correlations=.

- [X] Make it save the neuron_sort, neuron_notated_sort, and the
  pred_power
  - [X] Edit compute correlations hnb
  - [X] Copy stuff over to .py file
  - [X] Edit writing; I think it should use =torch.save= now..
  - [X] Copy stuff over to .py file

* TODO Edit analysis nb
  :PROPERTIES:
  :ARCHIVE_TIME: 2019-08-06 Tue 19:41
  :ARCHIVE_FILE: ~/core/mind/sys/short/proc/main.org
  :ARCHIVE_OLPATH: yonatan/contextual-corr-analysis/Create correlations measures out of corrs and linreg.
  :ARCHIVE_CATEGORY: main
  :ARCHIVE_TODO: TODO
  :END:
- [ ] Change loading, because the formats of the files are different now
- [ ] Allow using the correlations as similarity measures     
- [ ] 

This can wait.   

* TODO Regenerate output files for analysis-8
  :PROPERTIES:
  :ARCHIVE_TIME: 2019-08-06 Tue 19:41
  :ARCHIVE_FILE: ~/core/mind/sys/short/proc/main.org
  :ARCHIVE_OLPATH: yonatan/contextual-corr-analysis/Create correlations measures out of corrs and linreg.
  :ARCHIVE_CATEGORY: main
  :ARCHIVE_TODO: TODO
  :END:

Actually, this can wait.      

* DONE Create correlations measures out of corrs and linreg. 
  CLOSED: [2019-08-06 Tue 19:41]
  CLOCK: [2019-08-06 Tue 13:45]--[2019-08-06 Tue 13:47] =>  0:02
  :PROPERTIES:
  :ARCHIVE_TIME: 2019-08-06 Tue 19:41
  :ARCHIVE_FILE: ~/core/mind/sys/short/proc/main.org
  :ARCHIVE_OLPATH: yonatan/contextual-corr-analysis
  :ARCHIVE_CATEGORY: main
  :ARCHIVE_TODO: DONE
  :END:
 
 You could aggregate values in various ways -- mean, max. 
 
 There are more ways to use cca. For example, take the average of the
 top k cc values.

 Note: don't skip the pair between a layer and itself, because these
 won't be 1.

* DONE Prepare for meeting tomorrow morning
  CLOSED: [2019-08-06 Tue 19:41]
  :PROPERTIES:
  :ARCHIVE_TIME: 2019-08-06 Tue 19:41
  :ARCHIVE_FILE: ~/core/mind/sys/short/proc/main.org
  :ARCHIVE_OLPATH: yonatan/contextual-corr-analysis
  :ARCHIVE_CATEGORY: main
  :ARCHIVE_TODO: DONE
  :END:

* TODO Change everything to use pickling
  :PROPERTIES:
  :ARCHIVE_TIME: 2019-08-07 Wed 11:39
  :ARCHIVE_FILE: ~/core/mind/sys/short/proc/main.org
  :ARCHIVE_OLPATH: yonatan/contextual-corr-analysis
  :ARCHIVE_CATEGORY: main
  :ARCHIVE_TODO: TODO
  :END:
Will need to make everything become NumPy I think.     

* DONE Change everything to use pickling
    CLOSED: [2019-08-07 Wed 11:49]
    CLOCK: [2019-08-07 Wed 11:39]--[2019-08-07 Wed 11:49] =>  0:10
  :PROPERTIES:
  :ARCHIVE_TIME: 2019-08-07 Wed 11:49
  :ARCHIVE_FILE: ~/core/mind/sys/short/proc/main.org
  :ARCHIVE_OLPATH: yonatan/contextual-corr-analysis
  :ARCHIVE_CATEGORY: main
  :ARCHIVE_TODO: DONE
  :END:
Will need to make everything become NumPy I think.

Wait. When should I make it numpy? Perhaps just at the end.. actually,
it's not super important...

- [X] maxmin
- [X] linreg
- [X] cca
- [X] lincka
- [X] rbfcka

* TODO Have a look at results from hassan and nadir
  CLOCK: [2019-08-07 Wed 11:54]--[2019-08-07 Wed 12:34] =>  0:40
  :PROPERTIES:
  :ARCHIVE_TIME: 2019-08-07 Wed 13:49
  :ARCHIVE_FILE: ~/core/mind/sys/short/proc/main.org
  :ARCHIVE_OLPATH: yonatan/contextual-corr-analysis
  :ARCHIVE_CATEGORY: main
  :ARCHIVE_TODO: TODO
  :END:

* DONE Commit revised cca
    CLOSED: [2019-08-07 Wed 14:06]
  :PROPERTIES:
  :ARCHIVE_TIME: 2019-08-07 Wed 14:06
  :ARCHIVE_FILE: ~/core/mind/sys/short/proc/main.org
  :ARCHIVE_OLPATH: yonatan/contextual-corr-analysis
  :ARCHIVE_CATEGORY: main
  :ARCHIVE_TODO: DONE
  :END:
- [X] Overwrite previous commit    

* DONE if network==other_network in maxmin and linreg
    CLOSED: [2019-08-08 Thu 12:34]
  :PROPERTIES:
  :ARCHIVE_TIME: 2019-08-08 Thu 12:37
  :ARCHIVE_FILE: ~/core/mind/sys/short/proc/main.org
  :ARCHIVE_OLPATH: yonatan/contextual-corr-analysis
  :ARCHIVE_CATEGORY: main
  :ARCHIVE_TODO: DONE
  :END:
Consensus is that this is unnecessary. My email to Yonatan:

Well, maxmincorr is extracted from the correlation matrix, so it
contains strictly more information than just the index and correlation
of the most correlated neuron pair.

My point is that I don't think we should remove the line

            if network == other_network:
                continue

In MaxMinCorr. The reasoning is:
- If we did, I don't think it would tell us much. All the info we'd have
  would be pairs. The whole correlation matrix is strictly better.
- If we did and added the whole correlation matrix to the output for
  every pair, our output would be way too big, and most of it would be
  unused.
 
Thus, I feel like this would be better to add as a different method,
probably in a completely different script. Even semantically, the
=corr_methods.py= module is supposed to be comparing representations
across layers, not intra-layer representations. Also, intra-layer
representations will require completely different methods, because the
goal is no longer determining correlation.

You also mentioned doing this for LinReg. What exactly did you mean? I'm
not sure how to interpret regressing the rest of the layer onto a
particular neuron means. I feel like the interpretation would be the
opposite of the others---the higher the predictive power, the more
useless the neuron is, because it just means that it's direction can
already be expressed as a linear combination of others. This isn't
precise because the learning dynamics affects this, but again, I feel
like it has to be treated differently.

Also, it might screw up the neuron sorts. 

* DONE Reorganize the =slurm= directory
    CLOSED: [2019-08-08 Thu 15:47]
  :PROPERTIES:
  :ARCHIVE_TIME: 2019-08-08 Thu 15:47
  :ARCHIVE_FILE: ~/core/mind/sys/short/proc/main.org
  :ARCHIVE_OLPATH: yonatan/contextual-corr-analysis
  :ARCHIVE_CATEGORY: main
  :ARCHIVE_TODO: DONE
  :END:
- [X] Change 8 and 11 to use the mk_results1 style    

* TODO Why did minlinreg fail?
     CLOCK: [2019-08-08 Thu 18:49]--[2019-08-09 Fri 12:04] => 17:15
  :PROPERTIES:
  :ARCHIVE_TIME: 2019-08-08 Thu 18:56
  :ARCHIVE_FILE: ~/core/mind/sys/short/proc/main.org
  :ARCHIVE_OLPATH: yonatan/contextual-corr-analysis/Top k correlations in SVCCA?
  :ARCHIVE_CATEGORY: main
  :ARCHIVE_TODO: TODO
  :END:
Wait, I don't think it did.      

* DONE Rewrite maxmin and meanstd computations
     CLOSED: [2019-08-09 Fri 19:05]
     CLOCK: [2019-08-09 Fri 18:40]--[2019-08-09 Fri 19:05] =>  0:25
     CLOCK: [2019-08-08 Thu 17:24]--[2019-08-08 Thu 18:49] =>  1:25
  :PROPERTIES:
  :ARCHIVE_TIME: 2019-08-09 Fri 19:06
  :ARCHIVE_FILE: ~/core/mind/sys/short/proc/main.org
  :ARCHIVE_OLPATH: yonatan/contextual-corr-analysis/Visualizations: cmaps, more sentences, more neurons
  :ARCHIVE_CATEGORY: main
  :ARCHIVE_TODO: DONE
  :END:
I feel like many of these computations can be rewritten so they read
from files on disk, and operate on files on disk. I feel like this is
particularly true of simple statistics like mean, std, etc.

So, what should the format of such a file be?

{
    "mean": {model : means},
    "std" : {model : std},
    "max" : {model : max},
    "min" : {model : min},
}

Ahh, shit.

Waiting on jobid 7691. Great. Works?

Great. Waiting on jobid 7694. 

* DONE Convert all computers to ubuntu 18
   CLOSED: [2019-08-09 Fri 19:50]
   CLOCK: [2019-08-09 Fri 19:06]--[2019-08-09 Fri 19:50] =>  0:44
   CLOCK: [2019-08-09 Fri 18:03]--[2019-08-09 Fri 18:04] =>  0:01
   CLOCK: [2019-08-09 Fri 18:03]--[2019-08-09 Fri 18:03] =>  0:00
   CLOCK: [2019-08-09 Fri 18:03]--[2019-08-09 Fri 18:03] =>  0:00
   CLOCK: [2019-08-09 Fri 17:51]--[2019-08-09 Fri 18:03] =>  0:12
   CLOCK: [2019-08-09 Fri 15:34]--[2019-08-09 Fri 15:54] =>  0:20
   CLOCK: [2019-08-09 Fri 14:53]--[2019-08-09 Fri 15:16] =>  0:23
   CLOCK: [2019-08-09 Fri 12:04]--[2019-08-09 Fri 12:44] =>  0:40
  :PROPERTIES:
  :ARCHIVE_TIME: 2019-08-09 Fri 19:50
  :ARCHIVE_FILE: ~/core/mind/sys/short/proc/main.org
  :ARCHIVE_OLPATH: yonatan
  :ARCHIVE_CATEGORY: main
  :ARCHIVE_TODO: DONE
  :END:
Waiting on Jim to see if I can be added to =sls-sudoers=.

Waiting on installers on quad, beorn, and sugar to finish.

- [X] begin installer on chris's computer
Wait for that to finish
- [X] Login on all machines, make sure that /data/sls is accessible. 

* DONE [#A] Top k correlations in SVCCA?
    CLOSED: [2019-08-12 Mon 13:42]
    CLOCK: [2019-08-05 Mon 14:05]--[2019-08-05 Mon 14:22] =>  0:17
  :PROPERTIES:
  :ARCHIVE_TIME: 2019-08-12 Mon 13:42
  :ARCHIVE_FILE: ~/core/mind/sys/short/proc/main.org
  :ARCHIVE_OLPATH: yonatan/contextual-corr-analysis
  :ARCHIVE_CATEGORY: main
  :ARCHIVE_TODO: DONE
  :END:

I don't think it's useful to put code for it in =corr_methods.py=. I
should just load the canonical correlations, and do everything directly.

There is also an interesting idea: could I get the heatmap to change, as
you change the value of k?

Well, for PWCCA, this is not as straightforward. It would require more
computation. Also, how would you even do it?
** DONE Rerun for svcca
     CLOSED: [2019-08-06 Tue 12:02]
Running, as of Mon Aug  5 19:23:29 EDT 2019
** DONE Rerun for svcca
     CLOSED: [2019-08-07 Wed 13:27]
In progress
** DONE Fix pwcca stuff
     CLOSED: [2019-08-08 Thu 12:24]
     CLOCK: [2019-08-08 Thu 12:24]--[2019-08-08 Thu 12:24] =>  0:00
- [X] Create a test script     
- [X] Run the test script
- [X] Get results
- [X] Rerun
- [X] Get results
** DONE Regen nb for analysis-8
     CLOSED: [2019-08-09 Fri 18:39]
     CLOCK: [2019-08-09 Fri 18:04]--[2019-08-09 Fri 18:29] =>  0:25
     CLOCK: [2019-08-09 Fri 17:22]--[2019-08-09 Fri 17:51] =>  0:29
     CLOCK: [2019-08-09 Fri 15:56]--[2019-08-09 Fri 16:02] =>  0:06
     CLOCK: [2019-08-09 Fri 12:44]--[2019-08-09 Fri 14:53] =>  2:09
     CLOCK: [2019-08-08 Thu 12:24]--[2019-08-08 Thu 12:49] =>  0:25
Actually, I should regen analysis-8 entirely.
- [X] Run all methods again
- [X] Get results
  
Ahh, shit. minlinreg failed. Actually, I don't think it failed.

waiting on jobid 5275, which is maxlinreg. OK, everything looks good.
- [X] Regen
  - [X] Make test sbatchable
  - [X] Wait on results
    
    Wait, so is the sbatch finished? Yep
  - [X] Regen first for the test
    
    We need to get the analysis nb working again. 
    - [X] Create a generic =update= function that works for both pw and
      sv.
    - [X] Create interactive heatmaps for maxmincorr and linregcorr
      - [X] Reindex to make rows = columns
  - [X] Copy test over, regen
    
    Waiting on notebook analysis-8 to finish

* DONE CCA
      CLOSED: [2019-08-13 Tue 13:59]
      CLOCK: [2019-08-13 Tue 12:41]--[2019-08-13 Tue 13:59] =>  1:18
  :PROPERTIES:
  :ARCHIVE_TIME: 2019-08-13 Tue 13:59
  :ARCHIVE_FILE: ~/core/mind/sys/short/proc/main.org
  :ARCHIVE_OLPATH: yonatan/contextual-corr-analysis/Add the more models suggested by yonatan./Look at errors. Some errors may be due to memory; should try and fix these.
  :ARCHIVE_CATEGORY: main
  :ARCHIVE_TODO: DONE
  :END:
Shit... this may actually be coded wrong. Ahhhhhhh shit.

Okay, well would it be good now? What is taking up memory? Yeah, should
be good now. If I give 240GB, it should be all g. 


* DONE [#A] Evidence that RBFCKA is stable under different sizes of the dataset
    CLOSED: [2019-08-14 Wed 11:25]
    CLOCK: [2019-08-09 Fri 16:02]--[2019-08-09 Fri 17:20] =>  1:18
  :PROPERTIES:
  :ARCHIVE_TIME: 2019-08-14 Wed 11:25
  :ARCHIVE_FILE: ~/core/mind/sys/short/proc/main.org
  :ARCHIVE_OLPATH: yonatan/contextual-corr-analysis
  :ARCHIVE_CATEGORY: main
  :ARCHIVE_TODO: DONE
  :END:
- [X] How exactly would I get such evidence?
  
  My code currently supports taking the first k words of the corpus. I
  could run this with different values of k. However, the initial values
  would then be overloaded. I don't think this is a huge deal,
  though. Alternatively, I could take samples of size 10000. 
  
  I think what I'll do is just 5000, 10000, 15000, 20000. I don't think
  the evidence needs to be super statistically motivated; as long as we
  say we didn't see much of a difference, I think it's ok.
- [X] Setup everything for running =mk_results12.sh=. 
  - [X] Create the script, and dependencies
    - [X] Create mk_results12-helper.sh
    - [X] 
  - [X] Alter readme
- [X] Run mk_results12.sh
- [X] Get results
  
  Waiting on jobs 1098{4..7}
- [X] Why did it fail? Wait, what? time limit exceeded? Fuck...
- [X] How many iterations did it get through?
- [X] Shit.. Actually though, why is it even using dask? 
  - [X] Before that, change the limit to affect the original tensors
    being loaded in, NOT just during RBFCKA. 
    - [X] Remove the limit stuff from RBFCKA
    - [X] Add limit stuff in loading
      - [X] I should raise an error in the loading if the results are
        not all the same shape. Actually, nvm. That can be implemented
        as a separate function, =check_correct= or something. 
      - [X] sldkfj
    - [X] 
  - [X] I'll test again, we'll see
    
    11620       gpu mk_resul  johnmwu  R       0:30      1 sls-sm-6
    11621       gpu mk_resul  johnmwu  R       0:30      1 sls-sm-3
    11622       gpu mk_resul  johnmwu  R       0:30      1 sls-sm-3
    11623       gpu mk_resul  johnmwu  R       0:30      1 sls-titan-10
  - [X] Try again with some print statements. This is really weird that
    only 20000 doesn't work.
    
    Hmm... idk.. it seems to have taken 5h.
** DONE Create results12 with different filenames. fuck!
     CLOSED: [2019-08-14 Wed 10:56]
     CLOCK: [2019-08-14 Wed 10:54]--[2019-08-14 Wed 10:56] =>  0:02
Waiting on jobs 3049{2,3,4}     
** DONE Create an analysis notebook for RBFCKA. 
   CLOSED: [2019-08-14 Wed 11:25]
   CLOCK: [2019-08-14 Wed 10:56]--[2019-08-14 Wed 11:25] =>  0:29

* DONE [#A] Correlations of random activations
    CLOSED: [2019-08-14 Wed 13:42]
    CLOCK: [2019-08-14 Wed 13:23]--[2019-08-14 Wed 13:42] =>  0:19
    CLOCK: [2019-08-13 Tue 18:35]--[2019-08-13 Tue 18:43] =>  0:08
    CLOCK: [2019-08-13 Tue 14:05]--[2019-08-13 Tue 14:21] =>  0:16
  :PROPERTIES:
  :ARCHIVE_TIME: 2019-08-14 Wed 13:42
  :ARCHIVE_FILE: ~/core/mind/sys/short/proc/main.org
  :ARCHIVE_OLPATH: yonatan/contextual-corr-analysis
  :ARCHIVE_CATEGORY: main
  :ARCHIVE_TODO: DONE
  :END:

- [X] Create script     
  - [X] Create helper
    - [X] Create opt13 and repr_files13
  - [X] 
- [X] Edit readme
- [X] Get results
  
  Submitted batch job 30751
  Submitted batch job 30752
  Submitted batch job 30753
  Submitted batch job 30754
  Submitted batch job 30755
  Submitted batch job 30756
OK, I can now copy over the analysis nb. Ahh, shit! I fucked it up.

I need to somehow make =representations_d= able to use indices that are
other than just the basename of the dirname. The simplest solution is
just to change fname2mname.

Fuck. I'd thought that the structure would be more something like
=/elmo_rand/ptb_pos_dev.hdf5=
=/elmo/ptb_pos_dev.hdf5=
than what it currently is.

Okee, let's continue
- [X] Get results
  
  Submitted batch job 32135
  Submitted batch job 32136
  Submitted batch job 32137
  Submitted batch job 32138
  Submitted batch job 32139
  Submitted batch job 32140
- [X] Create analysis13 nb?


* DONE For RBFCKA, compute some statistics
  CLOSED: [2019-08-14 Wed 14:07]
  CLOCK: [2019-08-14 Wed 13:48]--[2019-08-14 Wed 14:07] =>  0:19
  :PROPERTIES:
  :ARCHIVE_TIME: 2019-08-14 Wed 14:07
  :ARCHIVE_FILE: ~/core/mind/sys/short/proc/main.org
  :ARCHIVE_OLPATH: yonatan/contextual-corr-analysis
  :ARCHIVE_CATEGORY: main
  :ARCHIVE_TODO: DONE
  :END:

  

* DONE Commit and push the images? That would help others that aren't here. 
  CLOSED: [2019-08-14 Wed 14:38]
  CLOCK: [2019-08-14 Wed 14:07]--[2019-08-14 Wed 14:38] =>  0:31
  :PROPERTIES:
  :ARCHIVE_TIME: 2019-08-14 Wed 14:38
  :ARCHIVE_FILE: ~/core/mind/sys/short/proc/main.org
  :ARCHIVE_OLPATH: yonatan/contextual-corr-analysis
  :ARCHIVE_CATEGORY: main
  :ARCHIVE_TODO: DONE
  :END:

* WAIT [#A] Regen CCA for everything that needs it.
    CLOCK: [2019-08-14 Wed 16:27]--[2019-08-14 Wed 16:29] =>  0:02
    CLOCK: [2019-08-14 Wed 16:26]--[2019-08-14 Wed 16:27] =>  0:01
    CLOCK: [2019-08-14 Wed 13:11]--[2019-08-14 Wed 13:23] =>  0:12
    CLOCK: [2019-08-13 Tue 14:00]--[2019-08-13 Tue 14:05] =>  0:05
  :PROPERTIES:
  :ARCHIVE_TIME: 2019-08-14 Wed 16:29
  :ARCHIVE_FILE: ~/core/mind/sys/short/proc/main.org
  :ARCHIVE_OLPATH: yonatan/contextual-corr-analysis
  :ARCHIVE_CATEGORY: main
  :ARCHIVE_TODO: WAIT
  :END:
Subsumed into another task

Waiting on 30749 and 30750

I guess this would be results 1 and 8

Oh... fuck me. Somehow, things are tensors again in CCA. I can't have
this. FUCK. Need to change CCA to save things as numpy arrays. Then run
again for 1 and 8.

Converting from torch tensor to np array: what ways are there to do
this? How about np.asarray or something? Maybe that's safer? Ahh,
whatever. We're sure we're using torch anyway.

Submitted batch job 46440
Submitted batch job 46441

Then we need to regen analysis-8. 

* DONE Regen analysis 8 after all new runs are complete
    CLOSED: [2019-08-15 Thu 11:56]
    CLOCK: [2019-08-14 Wed 16:27]--[2019-08-14 Wed 16:27] =>  0:00
  :PROPERTIES:
  :ARCHIVE_TIME: 2019-08-15 Thu 11:56
  :ARCHIVE_FILE: ~/core/mind/sys/short/proc/main.org
  :ARCHIVE_OLPATH: yonatan/contextual-corr-analysis
  :ARCHIVE_CATEGORY: main
  :ARCHIVE_TODO: DONE
  :END:
Submitted batch job 46444
Submitted batch job 46445
Submitted batch job 46446
Submitted batch job 46447
Submitted batch job 46448
Submitted batch job 46449

Redo mincorr
Submitted batch job 46545


* DONE [#A] Ask Jim whether there's anything he wants me to do.
  CLOSED: [2019-08-16 Fri 14:57]
  :PROPERTIES:
  :ARCHIVE_TIME: 2019-08-16 Fri 14:57
  :ARCHIVE_FILE: ~/core/mind/sys/short/proc/main.org
  :ARCHIVE_OLPATH: yonatan/contextual-corr-analysis
  :ARCHIVE_CATEGORY: main
  :ARCHIVE_TODO: DONE
  :END:

* DONE [#A] Writeup of what we've been doing, send to Jim
    CLOSED: [2019-08-16 Fri 16:01]
    CLOCK: [2019-08-16 Fri 15:25]--[2019-08-16 Fri 16:01] =>  0:36
  :PROPERTIES:
  :ARCHIVE_TIME: 2019-08-16 Fri 16:01
  :ARCHIVE_FILE: ~/core/mind/sys/short/proc/main.org
  :ARCHIVE_OLPATH: yonatan/contextual-corr-analysis
  :ARCHIVE_CATEGORY: main
  :ARCHIVE_TODO: DONE
  :END:
I guess I'll just do this in plain text?

I can include it as an update?

* TODO [#A] Email Yonatan about what time he wants to regroup
  :PROPERTIES:
  :ARCHIVE_TIME: 2019-09-19 Thu 17:19
  :ARCHIVE_FILE: ~/core/mind/sys/short/proc/main.org
  :ARCHIVE_OLPATH: yonatan
  :ARCHIVE_CATEGORY: main
  :ARCHIVE_TODO: TODO
  :END:

* DONE attn analysis
  CLOSED: [2019-09-24 Tue 11:38]
  CLOCK: [2019-09-24 Tue 11:11]--[2019-09-24 Tue 11:36] =>  0:25
  :PROPERTIES:
  :ARCHIVE_TIME: 2019-09-24 Tue 11:39
  :ARCHIVE_FILE: ~/core/mind/sys/short/proc/main.org
  :ARCHIVE_OLPATH: yonatan
  :ARCHIVE_CATEGORY: main
  :ARCHIVE_TODO: DONE
  :END:

* WAIT [#A] Add the more models suggested by yonatan.
  CLOCK: [2019-08-07 Wed 16:02]--[2019-08-07 Wed 16:35] =>  0:33
  CLOCK: [2019-08-07 Wed 13:50]--[2019-08-07 Wed 14:14] =>  0:24
  :PROPERTIES:
  :ARCHIVE_TIME: 2019-09-24 Tue 11:39
  :ARCHIVE_FILE: ~/core/mind/sys/short/proc/main.org
  :ARCHIVE_OLPATH: yonatan/contextual-corr-analysis
  :ARCHIVE_CATEGORY: main
  :ARCHIVE_TODO: WAIT
  :END:
** DONE Optimize linreg; svds only need to be computed once
     CLOSED: [2019-08-07 Wed 17:12]
- [X] Update the hnb to accurately reflect stuff     
- [X] Install the line profiler and see how much time the SVD takes
  compared to other things
- [X] Add line_profiler into anki
- [X] Make edits in hnb
- [X] Add changes to =corr_methods.py=.
** DONE Create tester scripts for each of the methods
   CLOSED: [2019-08-07 Wed 17:12]
** DONE Get tester scripts to run successfully
     CLOSED: [2019-08-07 Wed 18:10]
     CLOCK: [2019-08-07 Wed 17:19]--[2019-08-07 Wed 18:10] =>  0:51
     CLOCK: [2019-08-07 Wed 17:15]--[2019-08-07 Wed 17:18] =>  0:03
- [X] Run      
- [X] Can I load these files into Jupyter?
- [X] Try recreating analysis-8 as a test analysis nb
  - [X] Where should I put this analysis nb?
  - [X] 
** DONE Create new script
   CLOSED: [2019-08-07 Wed 18:54]
   CLOCK: [2019-08-07 Wed 18:10]--[2019-08-07 Wed 18:54] =>  0:44
** DONE Get results
   CLOSED: [2019-08-08 Thu 12:21]
** DONE Look at errors. Some errors may be due to memory; should try and fix these.
     CLOSED: [2019-08-13 Tue 13:59]

It seems that CCA and Linreg are running out of memory, regardless.
** WAIT Somehow, the shapes are wrong

* TODO Document, readme, finish up analyis
    CLOCK: [2019-08-16 Fri 11:05]--[2019-08-16 Fri 11:40] =>  0:35
    CLOCK: [2019-08-15 Thu 12:14]--[2019-08-15 Thu 14:19] =>  2:05
    CLOCK: [2019-08-14 Wed 18:56]--[2019-08-14 Wed 18:56] =>  0:00
    CLOCK: [2019-08-14 Wed 16:56]--[2019-08-14 Wed 17:46] =>  0:50
  :PROPERTIES:
  :ARCHIVE_TIME: 2019-09-24 Tue 11:39
  :ARCHIVE_FILE: ~/core/mind/sys/short/proc/main.org
  :ARCHIVE_OLPATH: yonatan/contextual-corr-analysis
  :ARCHIVE_CATEGORY: main
  :ARCHIVE_TODO: TODO
  :END:
- [X] Create a module =var.py= which houses functions which will need to
  be reimplemented by each user. 
- [X] Change first_half_only_l etc to be False by default, and allow
  these values in the function.
  - [X] Change hnb
  - [X] 
- [X] Is there a way to import the =var.py= file from my ju notebooks?
  
  Maybe I can have my environment automatically have the project as part
  of its path?
- [X] Define a network_sort_key function
- [X] regen analysis-13
- [X] I should write small docstrings for each Method
  object. Unfortunately, I'm not sure whether to put this under the
  Method object, or the __init__ function.
Okay, I'm pretty much done =corr_methods=.

- [X] Copy improvements in analysis-8 over to the test analysis nb. 
- [X] Answer Yonatan's requests
  - [X] sldkjf
- [ ] 

* TODO Have a look at yonatan's code in the repo
  :PROPERTIES:
  :ARCHIVE_TIME: 2019-09-24 Tue 11:39
  :ARCHIVE_FILE: ~/core/mind/sys/short/proc/main.org
  :ARCHIVE_OLPATH: yonatan/contextual-corr-analysis
  :ARCHIVE_CATEGORY: main
  :ARCHIVE_TODO: TODO
  :END:

* TODO Dynamic loading of tensors
    CLOCK: [2019-08-13 Tue 17:24]--[2019-08-13 Tue 17:48] =>  0:24
    CLOCK: [2019-08-13 Tue 16:36]--[2019-08-13 Tue 16:38] =>  0:02
     CLOCK: [2019-08-07 Wed 14:14]--[2019-08-07 Wed 16:02] =>  1:48
  :PROPERTIES:
  :ARCHIVE_TIME: 2019-09-24 Tue 11:40
  :ARCHIVE_FILE: ~/core/mind/sys/short/proc/main.org
  :ARCHIVE_OLPATH: yonatan/contextual-corr-analysis
  :ARCHIVE_CATEGORY: main
  :ARCHIVE_TODO: TODO
  :END:
This is annoying because it isn't testable through Jupyter. Wait. IS
it? Also, this might be method-specific.

Wait. I think my computer should support cuda9.. let's try installing pt
with cuda9 and seeing what happens.

It seems we need an older version of PyTorch. Ahh... didn't work.

Hmmm... maybe we try to do it on the GPU, and if it fails, then we
switch that iteration to the cpu. I think this is probably the cleanest.
** TODO What is the error that you get when the GPU memory runs out?
I can create a test?     
** TODO Create a gpu-memlimit option, that is passed to the method constructor.
** TODO Create a use_gpu function that somehow decides whether to use the gpu, given a bunch of information (for example, maybe just pass in the tensors directly)

* TODO Implement a correctness check (for shapes) before everything else
  :PROPERTIES:
  :ARCHIVE_TIME: 2019-09-24 Tue 11:40
  :ARCHIVE_FILE: ~/core/mind/sys/short/proc/main.org
  :ARCHIVE_OLPATH: yonatan/contextual-corr-analysis
  :ARCHIVE_CATEGORY: main
  :ARCHIVE_TODO: TODO
  :END:

* TODO I need to understand: how many params are in a transformer?
  :PROPERTIES:
  :ARCHIVE_TIME: 2019-09-24 Tue 11:40
  :ARCHIVE_FILE: ~/core/mind/sys/short/proc/main.org
  :ARCHIVE_OLPATH: yonatan/contextual-corr-analysis
  :ARCHIVE_CATEGORY: main
  :ARCHIVE_TODO: TODO
  :END:

* TODO Significance testing -- add some random noise
  :PROPERTIES:
  :ARCHIVE_TIME: 2019-09-24 Tue 11:41
  :ARCHIVE_FILE: ~/core/mind/sys/short/proc/main.org
  :ARCHIVE_OLPATH: yonatan/contextual-corr-analysis
  :ARCHIVE_CATEGORY: main
  :ARCHIVE_TODO: TODO
  :END:

* TODO Experiment with dask array, see if you can do anything with it. (w)
  :PROPERTIES:
  :ARCHIVE_TIME: 2019-09-24 Tue 11:41
  :ARCHIVE_FILE: ~/core/mind/sys/short/proc/main.org
  :ARCHIVE_OLPATH: yonatan/contextual-corr-analysis
  :ARCHIVE_CATEGORY: main
  :ARCHIVE_TODO: TODO
  :END:
** DONE Read some extra stuff about it
** DONE Create some testing section in my filesystem for it
** DONE Test some stuff
*** DONE Wow. Duck typing is beautiful.. I should write something
     about it..
*** DONE Add functionality to use dask in RBFCKA
**** DONE Copy past compute_correlations over
**** DONE Add dask_chunk_size capability in all scripts
***** DONE RBFCKA constructor
** DONE Alter scripts to be able to use dask arrays for RBF CKA.
*** DONE Add a dask chunk size flag nvm.
*** DONE Make scripts able to take in the limit
** TODO Try running it
*** DONE Create mk_results10-1.sh
*** DONE Determine if it actually uses the max number of cores it can
*** DONE Run
*** DONE Recv results
*** DONE Hmm.. maybe I need to be clearer on how to instantiate the
     client..
     
     Yes. I think so.
*** TODO Solve the following error
     
     OSError: [Errno 122] Disk quota exceeded: '/data/sls/u/urop/johnmwu/contextual-corr-analysis/slurm/dask-worker-space/worker-v9w3_pmk/storage/%28%27exp-2798cd0f343803b1a5311840159953ce%27%2C%201%2C%201%29'
     
     [[https://stackoverflow.com/q/40042748/4019495][this]] SO question seems to be related. I'm not sure if its the
     same error. Well, it's not, but perhaps changing my temporary
     directory will do something.
     
     I can try what this recommends
     
     task
**** DONE Fill in temporary_directory entry in yaml file
***** DONE What is a yaml file?
**** DONE Install pyyaml?
**** DONE Run again
**** DONE Receive results, hopefully if error it's a diff one. 
       
       Nope, again got an error...
       
       OSError: [Errno 122] Disk quota exceeded mk_results10-1.sh: line 7: 194830 Killed                  python ../main.py "${repr_files}" "${results}" --opt_fname "${opt_fname}" --methods rbfcka
**** TODO

* TODO Create LaTeX document with mathematical derivations
  :PROPERTIES:
  :ARCHIVE_TIME: 2019-09-24 Tue 11:41
  :ARCHIVE_FILE: ~/core/mind/sys/short/proc/main.org
  :ARCHIVE_OLPATH: yonatan/contextual-corr-analysis
  :ARCHIVE_CATEGORY: main
  :ARCHIVE_TODO: TODO
  :END:
 
 ideas
** Equivalence of our centering (in LinCKA) with theirs

* TODO Spearman significance testing
  :PROPERTIES:
  :ARCHIVE_TIME: 2019-09-24 Tue 11:41
  :ARCHIVE_FILE: ~/core/mind/sys/short/proc/main.org
  :ARCHIVE_OLPATH: yonatan/contextual-corr-analysis
  :ARCHIVE_CATEGORY: main
  :ARCHIVE_TODO: TODO
  :END:

* Visualizations: cmaps, more sentences, more neurons
    CLOCK: [2019-08-08 Thu 14:41]--[2019-08-08 Thu 17:24] =>  2:43
  :PROPERTIES:
  :ARCHIVE_TIME: 2019-09-24 Tue 11:41
  :ARCHIVE_FILE: ~/core/mind/sys/short/proc/main.org
  :ARCHIVE_OLPATH: yonatan/contextual-corr-analysis
  :ARCHIVE_CATEGORY: main
  :END:
- [X] Regen everything in results-1    
  - [X] Rewrite =mk_results1.sh=. I guess it should be an sbatch script
    instead?
  - [X] Run it
  - [X] Get results
  - [X] 
- [X] Rewrite vis notebook to reflect new data format

* TODO Write unit tests, test the code
  :PROPERTIES:
  :ARCHIVE_TIME: 2019-09-24 Tue 11:41
  :ARCHIVE_FILE: ~/core/mind/sys/short/proc/main.org
  :ARCHIVE_OLPATH: yonatan/contextual-corr-analysis
  :ARCHIVE_CATEGORY: main
  :ARCHIVE_TODO: TODO
  :END:
** TODO How do unit tests work in Python?
*** DONE What is the build passing thing you see on github?
     This is related to the notion of "Continuous Delivery". It is an
     interesting concept.
*** TODO
** TODO Write some tests
*** TODO TestMethods
*** TODO TestLoading
*** TODO TestWriting

* TODO [#B] Cleanup directory
  :PROPERTIES:
  :ARCHIVE_TIME: 2019-09-24 Tue 11:41
  :ARCHIVE_FILE: ~/core/mind/sys/short/proc/main.org
  :ARCHIVE_OLPATH: yonatan/contextual-corr-analysis
  :ARCHIVE_CATEGORY: main
  :ARCHIVE_TODO: TODO
  :END:

* TODO Write new proposal
  :PROPERTIES:
  :ARCHIVE_TIME: 2019-09-24 Tue 11:41
  :ARCHIVE_FILE: ~/core/mind/sys/short/proc/main.org
  :ARCHIVE_OLPATH: yonatan/contextual-corr-analysis
  :ARCHIVE_CATEGORY: main
  :ARCHIVE_TODO: TODO
  :END:
What are attention heads doing?    

* TODO Update =load_representations= to not be specific to this project
  :PROPERTIES:
  :ARCHIVE_TIME: 2019-09-24 Tue 11:41
  :ARCHIVE_FILE: ~/core/mind/sys/short/proc/main.org
  :ARCHIVE_OLPATH: yonatan/contextual-corr-analysis
  :ARCHIVE_CATEGORY: main
  :ARCHIVE_TODO: TODO
  :END:
For example, "sentence_to_index" is a bit too specific..

* contextual-corr-analysis
  CLOCK: [2019-08-05 Mon 11:13]--[2019-08-05 Mon 14:05] =>  2:52
  :PROPERTIES:
  :ARCHIVE_TIME: 2019-09-24 Tue 11:41
  :ARCHIVE_FILE: ~/core/mind/sys/short/proc/main.org
  :ARCHIVE_OLPATH: yonatan
  :ARCHIVE_CATEGORY: main
  :END:

* DONE Look at Yonatan's implementations of the attention corrs
  CLOSED: [2019-09-24 Tue 11:52]
  CLOCK: [2019-09-20 Fri 19:23]--[2019-09-20 Fri 19:36] =>  0:13
  CLOCK: [2019-09-19 Thu 16:23]--[2019-09-19 Thu 17:52] =>  1:29
  CLOCK: [2019-09-13 Fri 20:51]--[2019-09-13 Fri 21:29] =>  0:38
  CLOCK: [2019-09-13 Fri 19:53]--[2019-09-13 Fri 19:58] =>  0:05
  :PROPERTIES:
  :ARCHIVE_TIME: 2019-09-24 Tue 11:52
  :ARCHIVE_FILE: ~/core/mind/sys/short/proc/main.org
  :ARCHIVE_OLPATH: yonatan
  :ARCHIVE_CATEGORY: main
  :ARCHIVE_TODO: DONE
  :END:
** DONE What do the attn files contain?
    CLOSED: [2019-09-19 Thu 17:52]
- [X] where do I put this?
- [X] What is the other 12 dimension in bert_base? I'm guessing it's the
  number of attn heads?

  I think it's the number of attn heads. Can I find evid of this in the
  bert paper? Ahh, yes. 
- [X] What is the order of these? Is the first index the layer or the
  the index of the attn head?

  Actually, how would you even tell? Lol, just look at one where they're
  not equal.
- [X] What is the structure of the =n_word= \times =n_word= matrix?

  For instance, is it a stochastic matrix? Yes. So, this is Q^T K.
** TODO What attention corr methods has yonatan implemented?
    CLOCK: [2019-09-19 Thu 17:52]--[2019-09-19 Thu 18:04] =>  0:12
OK, so =attentions_l= is a list of num_heads x n_word x n_word tensors.

Wait, the mean frobenius norm isn't fair. Longer sentences will have a
smaller frobenius norm difference. Maybe I can propose a better
approach. 

The key question is: given two sequences of matrices, how what is the
"similarity" between those two sequences?

How would we evaluate a measure of attention similarity? Actually, we
can use the same thing CKA did---how well the similarity index can
predict. If M1, M2 are two of the same model trained from different
initializations, the first transformer head layer in M1 should be most
similar to the first transformer head layer in M2. 

Another thing we could think about is what the transformation measure
should be invariant under, and what it shouldn't.

Unfortunately, we don't have a big representation matrix to think
about. We can't take something like that and look at its eigvals or
something.
*** should be invariant
- swapping of attn heads. Actually, this might be about it. 
** misc
At some point, I said something like: 

#+BEGIN_QUOTE
Maybe we can do something like maxmincorr
#+END_QUOTE

What did I mean by this?

Previously, we were looking at layers of neurons. Now, we're looking at
layers of attn heads.

Do each of the methods make sense to transfer to attn?
- maxmincorr

  Yes, if we have a way to correlate two attn heads. In fact, it works
  the exact same way.
- linreg

  I don't think this makes sense. Here, we're fundamentally looking at
  linear combinations of attn heads. However, the attn heads are always
  separate. The first place where you can take linear combinations is
  when they factor into neuron activations again; if you're just looking
  at alignment matrices this makes no sense at all.
- svcca

  Again, subspace doesn't seem to make much sense.
- cka

  What is your kernel matrix? Again, I'm not too sure about this one.

* DONE Organize task list for project
   CLOSED: [2019-09-24 Tue 11:56]
   CLOCK: [2019-09-24 Tue 11:42]--[2019-09-24 Tue 11:56] =>  0:14
  :PROPERTIES:
  :ARCHIVE_TIME: 2019-09-24 Tue 11:56
  :ARCHIVE_FILE: ~/core/mind/sys/short/proc/main.org
  :ARCHIVE_OLPATH: yonatan
  :ARCHIVE_CATEGORY: main
  :ARCHIVE_TODO: DONE
  :END:
What separations do I want to make?
- tasks vs things to learn

  within this, tasks yonatan specifically mentions vs tasks I want to
  do.
- 

* DONE Formalize notion of distributed vs localized
    CLOSED: [2019-10-03 Thu 10:23]
  :PROPERTIES:
  :ARCHIVE_TIME: 2019-10-03 Thu 10:23
  :ARCHIVE_FILE: ~/core/mind/sys/short/proc/main.org
  :ARCHIVE_OLPATH: yonatan/proj
  :ARCHIVE_CATEGORY: main
  :ARCHIVE_TODO: DONE
  :END:
Shit.. searching for "distributed vs localized" online doesn't result in
anything.

Maybe I can find the paper I read that talked about it?

I guess I roughly already did this..

I sent an email to Yonatan with my idea about this. 
