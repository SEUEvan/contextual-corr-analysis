* Question
- [X] In the =sentence_d= variable, why are they (sentence, string index) pairs by
  default? See =load_embedding_hnb= *YB*: the hdf5 file structure comes from
  [[https://github.com/nelson-liu/contextual-repr-analysis/blob/master/contexteval/contextualizers/precomputed_contextualizer.py][Nelson's code]]. Maybe going through the string is a sanity check to make sure
  we're consistent with the same sentences. Feel free to use it differently
  though.
- [X] How do I interpret the embeddings? See =load_embeddings_hnb= I think these are
  num_layers x len(sentence) x num_neurons. I need to verify. *YB: yes, this is
  correct*
  - [X] num_layers
  - [X] len(sentence)
    Fairly certain
  - [X] num_neurons
- [X] Is there a reason for the =first_half_only= and =second_half_only= 
  Seems kind of arbitrary to me. *YB*: yes, this is for
  ELMo, where the first half is the forward RNN representations and the second
  half is the backward ENN representations. In practice, they are often
  concatenated and treated together, but having the option to separate them is
  useful.
- [X] What does the following line of code mean:
  #+BEGIN_SRC python
    activations = torch.cat([torch.stack([torch.cat(token) for token in sentence])
                             for sentence in activations]).cpu() 
    self.activations[fname] = activations
  #+END_SRC
  I'm particularly confused about the first line -- what is trying to be
  accomplished here? For the moment (Mon Jun 17 13:06:56 EDT 2019) I'll just
  replace the first line with torch.cat(activations), because that's what I
  think is meant. 
  
  *YB*: no, that would be wrong. The
  expression "torch.stack([torch.cat(token) for token in sentence]" concatenates
  a list of tokens along a new dimension (that's from the stack method). Then,
  all these new tensors are concatenated along the same dimension. 

  *MW*: Still kind of confused. The expression torch.cat(token) is not valid;
  =token= is a tensor, so you can't =torch.cat= it. Could you let me know
  exactly what sort of tensor is desired?  

  As it is, torch.cat(activations) (or now, torch.cat(representations_l)) is a
  shape (num_words, num_neurons) tensor, where num_words is the number of words
  in the corpus. This is exactly what we need to perform correlations among
  different neurons, no? See cell 15 of the hnb. Of course, what I did gets rid
  of the information regarding what sentence a particular representation came
  from.

  Ok, as of Mon Jun 24 17:16:49 EDT 2019, we're deleting this. 
- [ ] Mon Jun 17 14:14:52 EDT 2019 Style wise, is everything ok? 
   Answer for the the updated =load_embeddings= function
- [X] In the Method class, we provide the ability to specify how things are
  loaded (first half only, etc.). However, in the main.py script, these options
  are unavailable. Why is this?
- [X] For the Linear Regression method of correlation, why do we use MSE? I feel
  like this biases toward neurons which have smaller activations. Suppose a
  particular neuron, on 3 words, has activations [0.1, 0.2, -0.1], and another
  has activations [10, 0, -10]. A "bad" linreg prediction of the first may have
  a better MSE than a "good" linreg prediction of the second (determined by
  R^2). I feel like ranking by R^2 makes more sense. 

  Ok, it's resolved. 
- [ ] For Linear Regression, there's a bias. 

  Specifically, it's easier to fit a model with more neurons to any given other
  model. In general, the fitting a 1000 neuron model will have a lower MSE than
  fitting a 100 neuron model to a given set of activations. Thus, when we take
  maximums of the MSE, it could be that most of them are fitting from one
  specific other model. 
- [ ] In SVCCA, how to sort?

  It seems we are writing down the transformation matrices. Ultimately though,
  don't we need to order some things by importance? Sort the svcca directions,
  or something? The SVCCA paper mentions an "svcca score". Is this what we will use?
- [ ] What does it mean to be "centered"? 

  This is mentioned in the cka paper. They don't make it clear whether it means
  that we are centered for each neuron, or that we are centered for each
  representation. I think it means the former (makes more sense, simpler), but
  idk.
- [ ] Can we change calls to torch.mm to the =@= operator?
- [ ] Not a problem, but why are some saves json, and others =torch.save=?
- [ ] When scheduling SLURM jobs, how to specify that a GPU should be used?
  Will PyTorch tensor operations automatically use the GPU if one is available?
  If not, then should I add this default to the code?
* Misc
- [X] As it is (Mon Jun 17 12:41:34 EDT 2019), the data will be loaded
  on each call of the script. As far as I can tell, the data loading is
  a significant portion of the runtime, so this may not be the
  smartest. Maybe change the architecture later. *YB*: agree, especially
  since we may want to apply all correlation methods on the same data.

  As of commit 1fc4e6f, this is resolved. 
- [X] Is there a reason why we need a "Method" class? Why can't
  everything be unrolled into a single, large script? *YB*: eventually,
  we may want to call these from another program (e.g., NeuroX) so
  having a separate file for defining the methods which is different
  from where they are called would be useful.

  Ok, I guess what I was wondering was more like why we need classes at all. The
  answer is most likely just because =compute_correlations= and
  =write_correlations=, things defined for each Method, are coupled. It would be
  weird to have 10 functions like =CKA_write=, =CKA_compute=, =LinReg_write=,
  =LinReg_compute=, etc. all in the global namespace. 
- [X] Maybe clean up the comments at some point (look more
  professional).
- In the CKA paper, they suggest different kernels. 
- MaxMinCorr is very inelegantly programmed. Maybe change at some point. 

  For instance, we compute a whole ton of intermediate quantities:
  =self.clusters=, =self.neuron_sort=. Do we need these? If our goal is only to
  create the object =self.neuron_notated_sort=, it can be done much more
  cleanly. 
- Maybe add a makefile to the test directory. 
- Ideas 
  Written Fri Jun 28 17:04:24 EDT 2019
  - Test whether these neurons are actually important.  This may be
    difficult, as these are upstream models. However, I think that
    this could be doable, but we need to choose the e2e model well. 
  - Determine whether there are particular neurons that capture
    specific linguistic information. An interesting thing to look
    at would be activations on the same word, in different
    contexts. This is already explored in some of the papers.
  - Do the contextual word representations also learn a better
    representation of words, regardless of context? How does the
    mean of all representations for a word perform?
  - What things do highly correlated neurons learn? Assuming highly
    correlated neurons are important, then what are these neurons
    learning?
  - What are features that model X learns, but model Y doesn't?
- Papers read
  - Identifying and Controlling Important Neurons.. 
  - Neural Machine Translation by Jointly Learning... (attn)
  - Similarity of Neural Network Representations Revisited (cka)
  - Deep Contextualized word representations (elmo)
  - Sequence to sequence learning with neural networks 
  - SVCCA: Singular Vector Canonical .. 
* Progress update
** Fri Jun 14 14:53:49 EDT 2019
In emacs org-mode, there should be a way to set it up so that when tasks are
completed, they are automatically appended to some file. I have yet to set this
up, so I'll just describe in words what I did. I may end up just doing things
this way. 

This week, I: (not necessarily in this order)
- Configured my environment (and settled in)
  - bash, emacs, etc.
  - conda
- Learned about PyTorch
  - 60 min blitz
  - "Deep Learning for NLP with Pytorch"
- Did some reading
- Began coding 

Hopefully, by the end of today, I'll make a commit. 

You were right, I really like PyTorch. I think its define-by-run semantics
is an especially neat idea. I also like how it seems to expose just the right
amount of detail to the end user (leading to pseudocode-like cleanness, but
still extremely configurable). 

I anticipate that I should be able to finish coding contextual-corr-analysis by
next week. It is not a lot, but I'm taking some time to get used to the modules
used. 

By the way, I heard you telling Jim that your brother is getting married. Have a
good time!
** Tue Jun 25 15:34:54 EDT 2019
Just finished coding the `compute_correlations` for everything. Last week, I
again spent a lot of time (3-4 out of 5 days) reading papers. 

Estimated that I'd finish the script by last week. Looks like that'll be today,
as the write_correlations method is not complex. May add some unit tests,
although I'm reasonably confident in correctness (from the helper
notebooks). Please have a brief glance. 

Please let me know what you want me to do next. Should I now attempt to run the
code on the files you gave me? How should I go about doing this?
* Goal
Not sure. 

Written Fri Jun 28 17:05:08 EDT 2019

What is the fundamental thing we're doing? We're trying to
understand how these contextualizers work. We are doing this
using correlations. "Understand", here, is the loaded word. What
could this mean? In fact, this ties back into the question: what
does it mean to comprehend an object?
- Decomposition. This is the view of "understanding" proposed by
  category theorists.

  The ideal feature mapping is one in which each axis represents
  a single property, and the codomain of that property is a
  totally ordered set. This would be like neuron 1 captures
  sentiment, neuron 2 captures tense, etc. One can impose order
  on both of these. There should be no correlation between
  these. Some properties are inherently categorical, with no
  order. These, I suppose, must get their own axis. 

  A slightly worse property would be to have basically this, but
  things not be axis aligned.
- Make predictions about it. 

  Don't like this. This is a conventional definition of
  "understanding". However, someone may know both languages, and
  can translate between them. Ideally, their output should be
  similar to the model's. Does this mean they understand the
  model?

  Perhaps make predictions about when it won't work. 
- Make small changes to alter the behavior. 



