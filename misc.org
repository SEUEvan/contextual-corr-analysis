* Question
- [X] In the =sentence_d= variable, why are they (sentence, string index) pairs by
  default? See =load_embedding_hnb= *YB*: the hdf5 file structure comes from
  [[https://github.com/nelson-liu/contextual-repr-analysis/blob/master/contexteval/contextualizers/precomputed_contextualizer.py][Nelson's code]]. Maybe going through the string is a sanity check to make sure
  we're consistent with the same sentences. Feel free to use it differently
  though.

- [X] How do I interpret the embeddings? See =load_embeddings_hnb= I think these are
  num_layers x len(sentence) x num_neurons. I need to verify. *YB: yes, this is
  correct*
  - [X] num_layers
  - [X] len(sentence)
    Fairly certain
  - [X] num_neurons

- [ ] Is there a reason for the =first_half_only= and =second_half_only= variables?
  Seems kind of arbitrary to me. *YB*: yes, this is for ELMo, where the first half is the forward RNN representations and the second half is the backward ENN representations. In practice, they are often concatenated and treated together, but having the option to separate them is useful. 

- [ ] What does the following line of code mean:
  #+BEGIN_SRC python
    activations = torch.cat([torch.stack([torch.cat(token) for token in sentence])
                             for sentence in activations]).cpu() 
    self.activations[fname] = activations
  #+END_SRC
  I'm particularly confused about the first line -- what is trying to be
  accomplished here? For the moment (Mon Jun 17 13:06:56 EDT 2019) I'll just
  replace the first line with torch.cat(activations), because that's what I
  think is meant. *YB*: no, that would be wrong. The expression "torch.stack([torch.cat(token) for token in sentence]" concatenates a list of tokens along a new dimension (that's from the stack method). Then, all these new tensors are concatenated along the same dimension. 
* Misc
- As it is (Mon Jun 17 12:41:34 EDT 2019), the data will be loaded on each call
  of the script. As far as I can tell, the data loading is a significant portion
  of the runtime, so this may not be the smartest. Maybe change the architecture
  later. *YB*: agree, especially since we may want to apply all correlation methods on the same data. 
- Is there a reason why we need a "Method" class? Why can't everything be
  unrolled into a single, large script? *YB*: eventually, we may want to call these from another program (e.g., NeuroX) so having a separate file for defining the methods which is different from where they are called would be useful. 

* Progress update
** Fri Jun 14 14:53:49 EDT 2019
In emacs org-mode, there should be a way to set it up so that when tasks are
completed, they are automatically appended to some file. I have yet to set this
up, so I'll just describe in words what I did. I may end up just doing things
this way. 

This week, I: (not necessarily in this order)
- Configured my environment (and settled in)
  - bash, emacs, etc.
  - conda
- Learned about PyTorch
  - 60 min blitz
  - "Deep Learning for NLP with Pytorch"
- Did some reading
- Began coding 

Hopefully, by the end of today, I'll make a commit. 

You were right, I really like PyTorch. I think its define-by-run semantics
is an especially neat idea. I also like how it seems to expose just the right
amount of detail to the end user (leading to pseudocode-like cleanness, but
still extremely configurable). 

I anticipate that I should be able to finish coding contextual-corr-analysis by
next week. It is not a lot, but I'm taking some time to get used to the modules
used. 

By the way, I heard you telling Jim that your brother is getting married. Have a
good time!
