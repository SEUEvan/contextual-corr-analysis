* Question
- [X] In the =sentence_d= variable, why are they (sentence, string
  index) pairs by default? 
  
  See =load_embedding_hnb= *YB*: the hdf5 file structure comes from
  [[https://github.com/nelson-liu/contextual-repr-analysis/blob/master/contexteval/contextualizers/precomputed_contextualizer.py][Nelson's code]]. Maybe going through the string is a sanity check to
  make sure we're consistent with the same sentences. Feel free to use
  it differently though.
- [X] How do I interpret the embeddings? See =load_embeddings_hnb= I
  think these are num_layers x len(sentence) x num_neurons. I need to
  verify. *YB: yes, this is correct*
  - [X] num_layers
  - [X] len(sentence)
    Fairly certain
  - [X] num_neurons
- [X] Is there a reason for the =first_half_only= and =second_half_only= 
  Seems kind of arbitrary to me. *YB*: yes, this is for
  ELMo, where the first half is the forward RNN representations and the second
  half is the backward ENN representations. In practice, they are often
  concatenated and treated together, but having the option to separate them is
  useful.
- [X] What does the following line of code mean:
  #+BEGIN_SRC python
    activations = torch.cat([torch.stack([torch.cat(token) for token in sentence])
                             for sentence in activations]).cpu() 
    self.activations[fname] = activations
  #+END_SRC
  I'm particularly confused about the first line -- what is trying to be
  accomplished here? For the moment (Mon Jun 17 13:06:56 EDT 2019) I'll just
  replace the first line with torch.cat(activations), because that's what I
  think is meant. 
  
  *YB*: no, that would be wrong. The
  expression "torch.stack([torch.cat(token) for token in sentence]" concatenates
  a list of tokens along a new dimension (that's from the stack method). Then,
  all these new tensors are concatenated along the same dimension. 

  *MW*: Still kind of confused. The expression torch.cat(token) is not valid;
  =token= is a tensor, so you can't =torch.cat= it. Could you let me know
  exactly what sort of tensor is desired?  

  As it is, torch.cat(activations) (or now, torch.cat(representations_l)) is a
  shape (num_words, num_neurons) tensor, where num_words is the number of words
  in the corpus. This is exactly what we need to perform correlations among
  different neurons, no? See cell 15 of the hnb. Of course, what I did gets rid
  of the information regarding what sentence a particular representation came
  from.

  Ok, as of Mon Jun 24 17:16:49 EDT 2019, we're deleting this. 
- [ ] Mon Jun 17 14:14:52 EDT 2019 Style wise, is everything ok? 
   Answer for the the updated =load_embeddings= function
- [X] In the Method class, we provide the ability to specify how things are
  loaded (first half only, etc.). However, in the main.py script, these options
  are unavailable. Why is this?
- [X] For the Linear Regression method of correlation, why do we use MSE? I feel
  like this biases toward neurons which have smaller activations. Suppose a
  particular neuron, on 3 words, has activations [0.1, 0.2, -0.1], and another
  has activations [10, 0, -10]. A "bad" linreg prediction of the first may have
  a better MSE than a "good" linreg prediction of the second (determined by
  R^2). I feel like ranking by R^2 makes more sense. 

  Ok, it's resolved. 
- [ ] For Linear Regression, there's a bias. 

  Specifically, it's easier to fit a model with more neurons to any given other
  model. In general, the fitting a 1000 neuron model will have a lower MSE than
  fitting a 100 neuron model to a given set of activations. Thus, when we take
  maximums of the MSE, it could be that most of them are fitting from one
  specific other model. 
- [ ] In SVCCA, how to sort?

  It seems we are writing down the transformation matrices. Ultimately though,
  don't we need to order some things by importance? Sort the svcca directions,
  or something? The SVCCA paper mentions an "svcca score". Is this what we will use?
- [ ] What does it mean to be "centered"? 

  This is mentioned in the cka paper. They don't make it clear whether it means
  that we are centered for each neuron, or that we are centered for each
  representation. I think it means the former (makes more sense, simpler), but
  idk.
- [ ] Can we change calls to torch.mm to the =@= operator?
- [ ] Not a problem, but why are some saves json, and others =torch.save=?
- [ ] When scheduling SLURM jobs, how to specify that a GPU should be used?
  Will PyTorch tensor operations automatically use the GPU if one is available?
  If not, then should I add this default to the code?
* Misc
- Allocation of 32 cores on a 630 machine
  
  Mon Jul 22 19:02:11 EDT 2019
  - According to =top=, rbfcka is indeed using multiple cores. The cpu
    usage seems to fluctuate from 1000 - 1800% cpu usage. This is on the
    cpu without using dask.
  - Thus, not sure if the allocation of 32 cores is entirely necessary.
  - job 10 (with dask) uses up to 5500% of cpu. Not sure how this is possible
- Mem usage
  
  Got this error:
  /usr/users/johnmwu/anaconda3/envs/jmw0/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.

- Currently, my rule of thumb is that everything saved should be on the CPU. 
- RBF CKA on entire dataset runs out of memory..
  
  Ran on Thu Jul 18 18:44:49 EDT 2019
  Computing correlations
  For method:  <corr_methods.RBFCKA object at 0x7f9056156f60>
  Traceback (most recent call last):
    File "../main.py", line 118, in <module>
      disable_cuda=args.disable_cuda) 
    File "../main.py", line 98, in main
      method.compute_correlations()
    File "/data/sls/u/urop/johnmwu/contextual-corr-analysis/corr_methods.py", line 564, in compute_correlations
      Gx = center_gram(gram_rbf(self.representations_d[network][:limit]))
    File "/data/sls/u/urop/johnmwu/contextual-corr-analysis/corr_methods.py", line 539, in gram_rbf
      return torch.exp(-sq_distances / (2*threshold**2 * sq_median_distance))
  RuntimeError: [enforce fail at CPUAllocator.cpp:56] posix_memalign(&data, gAlignment, nbytes) == 0. 12 vs 0

- I don't think these can even run on CUDA.. CUDA does not have 30GiB of
  mem..
- [X] As it is (Mon Jun 17 12:41:34 EDT 2019), the data will be loaded
  on each call of the script. As far as I can tell, the data loading is
  a significant portion of the runtime, so this may not be the
  smartest. Maybe change the architecture later. *YB*: agree, especially
  since we may want to apply all correlation methods on the same data.

  As of commit 1fc4e6f, this is resolved. 
- [X] Is there a reason why we need a "Method" class? Why can't
  everything be unrolled into a single, large script? *YB*: eventually,
  we may want to call these from another program (e.g., NeuroX) so
  having a separate file for defining the methods which is different
  from where they are called would be useful.

  Ok, I guess what I was wondering was more like why we need classes at all. The
  answer is most likely just because =compute_correlations= and
  =write_correlations=, things defined for each Method, are coupled. It would be
  weird to have 10 functions like =CKA_write=, =CKA_compute=, =LinReg_write=,
  =LinReg_compute=, etc. all in the global namespace. 
- [X] Maybe clean up the comments at some point (look more
  professional).
- In the CKA paper, they suggest different kernels. 
- MaxMinCorr is very inelegantly programmed. Maybe change at some point. 

  For instance, we compute a whole ton of intermediate quantities:
  =self.clusters=, =self.neuron_sort=. Do we need these? If our goal is only to
  create the object =self.neuron_notated_sort=, it can be done much more
  cleanly. 
- How to do ELMo
  
  ELMo is recommended as taking a linear combination of layers. 
- Maybe add a makefile to the test directory. 
- Ideas 
  Written Fri Jun 28 17:04:24 EDT 2019
  - Test whether these neurons are actually important.  This may be
    difficult, as these are upstream models. However, I think that
    this could be doable, but we need to choose the e2e model well. 
    
    We could compare these for different downstream tasks. 
  - Determine whether there are particular neurons that capture
    specific linguistic information. An interesting thing to look
    at would be activations on the same word, in different
    contexts. This is already explored in some of the papers.
  - Do the contextual word representations also learn a better
    representation of words, regardless of context? How does the
    mean of all representations for a word perform?
  - What things do highly correlated neurons learn? Assuming highly
    correlated neurons are important, then what are these neurons
    learning?
  - What are features that model X learns, but model Y doesn't?
  Written Mon Jul  8 12:50:31 EDT 2019
  - Values of attention heads
  - Values of the cell of the lstm.
    
    For some of these models, this could be harder than others. For
    example, ELMo. In ELMo, Yonatan says that the LSTM is all very low
    level, so extracting information out of the gates is not very easy. 
  - Also gates. Gates could be doing similar things to attention heads.
  - Maybe different contextualizers. For example, elmo trained w/ diff
    objectives. Maybe elmo on A is more correlated to bert on A than
    elmo on A with elmo on B. Also maybe xlnet or gpt2 (different
    models). We don't have activations. Use huggingface interface maybe.
  written Tue Jul 23 15:56:22 EDT 2019
  - is information centralized or distributed?
  - Based on this, you could perform some intervention experiments
  - Question of distributivity vs localization
    
    There's multiple ways to tackle this:
    - We can say a model is distributed for a property
    - We may also be able to try and say a network is "distributed" or
      "localized". This is harder, according to Yonatan, but can be done
      in an analogous way to looking at the singular values of a matrix
      or the gini coefficient of something. 
  - Finding the strengths of various models, and attempting to create a
    better model
    
    This is hard if we're restricted to unsupervised methods. 
  written Thu Jul 25 12:35:57 EDT 2019
  - in terms of improving models, deeper layers' attention heads maybe
    don't need to completely relearn alignments. If pos 5 of layer 2 is
    super aligned to pos 10 of layer 1, then the network maybe shouldn't
    have to completely figure this out again. Maybe the alignment model
    can have residual connections.
    
    Would be interesting if we could get any evidence that this is the
    case.
    
    Then again, different attention heads are learning different things,
    and forcing one attention head to be similar to the next may not be
    the best.
  - how correlated is the alignment model of one layer with the next?
  written Mon Jul 29 13:59:05 EDT 2019
  - better understanding the differences between LSTMs and Transformers
  - LSTM's cannot be "it" as far as NLP is concerned. 
    
    There are too many long range dependecies. For example, as I'm
    reading this paper, I'm operating kind of like an LSTM. But once I
    get to this phrase "the dataset" I think "wait, where is this
    dataset coming from again?", and then backtrack to look for
    it. Thus, when I read something, it's a combination of an LSTM and
    something else that is able to find long range connections. LSTMs
    are advertised as having long memory, but it's not long enough. As
    another example, why do you think humans need to revise before
    exams? The issue is that things are not just perfectly integrated
    into your knowledge as you become aware of them.
      
    Physically speaking, there's some sense in which humans *must*
    operate as a state machine (LSTM-like). In a deterministic view,
    the only things that can change you are external stimuli. But
    intuitively, all humans need to review from time to time, to
    reorganize.
    
    There may be papers that talk about this already. Would be great to
    find evidence of this.

- Papers read
  - Identifying and Controlling Important Neurons.. 
  - Neural Machine Translation by Jointly Learning... (attn)
  - Similarity of Neural Network Representations Revisited (cka)
  - Deep Contextualized word representations (elmo)
  - Sequence to sequence learning with neural networks 
  - SVCCA: Singular Vector Canonical .. 
  - Neural Machine Translation of Rare Words with Subword Units
  - Attention is all you need
  - Transformer-XL
  - XLnet
  - Linguistic Knowledge and Transferability of Contextual
    Representations
  - Building a Large Annotated Corpus of English: The Penn Treebank
  - CoNLL-2000 shared task
  - CoNLL-2003 shared task
  - SNLI
  - GloVe
* Progress update
** Fri Jun 14 14:53:49 EDT 2019
In emacs org-mode, there should be a way to set it up so that when tasks are
completed, they are automatically appended to some file. I have yet to set this
up, so I'll just describe in words what I did. I may end up just doing things
this way. 

This week, I: (not necessarily in this order)
- Configured my environment (and settled in)
  - bash, emacs, etc.
  - conda
- Learned about PyTorch
  - 60 min blitz
  - "Deep Learning for NLP with Pytorch"
- Did some reading
- Began coding 

Hopefully, by the end of today, I'll make a commit. 

You were right, I really like PyTorch. I think its define-by-run semantics
is an especially neat idea. I also like how it seems to expose just the right
amount of detail to the end user (leading to pseudocode-like cleanness, but
still extremely configurable). 

I anticipate that I should be able to finish coding contextual-corr-analysis by
next week. It is not a lot, but I'm taking some time to get used to the modules
used. 

By the way, I heard you telling Jim that your brother is getting married. Have a
good time!
** Tue Jun 25 15:34:54 EDT 2019
Just finished coding the `compute_correlations` for everything. Last week, I
again spent a lot of time (3-4 out of 5 days) reading papers. 

Estimated that I'd finish the script by last week. Looks like that'll be today,
as the write_correlations method is not complex. May add some unit tests,
although I'm reasonably confident in correctness (from the helper
notebooks). Please have a brief glance. 

Please let me know what you want me to do next. Should I now attempt to run the
code on the files you gave me? How should I go about doing this?
* Goal
Not sure. 

Written Fri Jun 28 17:05:08 EDT 2019

What is the fundamental thing we're doing? We're trying to
understand how these contextualizers work. We are doing this
using correlations. "Understand", here, is the loaded word. What
could this mean? In fact, this ties back into the question: what
does it mean to comprehend an object?
- Decomposition. This is the view of "understanding" proposed by
  category theorists.

  The ideal feature mapping is one in which each axis represents
  a single property, and the codomain of that property is a
  totally ordered set. This would be like neuron 1 captures
  sentiment, neuron 2 captures tense, etc. One can impose order
  on both of these. There should be no correlation between
  these. Some properties are inherently categorical, with no
  order. These, I suppose, must get their own axis. 

  A slightly worse property would be to have basically this, but
  things not be axis aligned.
- Make predictions about it. 

  Don't like this. This is a conventional definition of
  "understanding". However, someone may know both languages, and
  can translate between them. Ideally, their output should be
  similar to the model's. Does this mean they understand the
  model?

  Perhaps make predictions about when it won't work. 
- Make small changes to alter the behavior. 



